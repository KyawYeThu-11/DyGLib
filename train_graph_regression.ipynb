{"cells":[{"cell_type":"markdown","metadata":{"id":"SrOiGmrTQBmv"},"source":["# Environmental Setup\n","\n","\n","> Since it is impractical to download our dataset of sheer size in a reasonable time frame, we decided to have our implementations utilize the shared folder, which can be accessible from the mounted google drive. Therefore, for reproducibility, it is **IMPORTANT** to\n","1. Visit this [drive folder](https://drive.google.com/drive/folders/1VMn57KmlJ20DlviBlGufDC7vgdWIR9ni?usp=sharing).\n","2. Once visited, it'll show up in 'Shared with me' section in your google drive, from which you can add the shortcut to your drive.\n","3. Then, the shortcut should have the path, `drive/MyDrive/CS471 Project`.\n","\n","Mount Google Drive to the Colab VM and install necessary modules"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ekk-FV3Hn40y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716733405518,"user_tz":-540,"elapsed":19035,"user":{"displayName":"Kyaw Ye Thu","userId":"03677166176861607126"}},"outputId":"96601f45-d3bd-4dba-fc0a-266cc911223b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"s1YuxS8qTA17","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716733520758,"user_tz":-540,"elapsed":115243,"user":{"displayName":"Kyaw Ye Thu","userId":"03677166176861607126"}},"outputId":"cce862c2-08ca-4dc1-d618-d6364071e01b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting wget\n","  Downloading wget-3.2.zip (10 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: wget\n","  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=a82b4476e3bc0e97da5abf8e052d97a57447d6611987013e106c817807e4c90c\n","  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n","Successfully built wget\n","Installing collected packages: wget\n","Successfully installed wget-3.2\n","/content\n","Cloning into 'DyGLib'...\n","remote: Enumerating objects: 232, done.\u001b[K\n","remote: Counting objects: 100% (175/175), done.\u001b[K\n","remote: Compressing objects: 100% (93/93), done.\u001b[K\n","remote: Total 232 (delta 116), reused 119 (delta 81), pack-reused 57\u001b[K\n","Receiving objects: 100% (232/232), 16.50 MiB | 11.21 MiB/s, done.\n","Resolving deltas: 100% (128/128), done.\n","Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from -r DyGLib/requirements.txt (line 1)) (2.3.0+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r DyGLib/requirements.txt (line 2)) (1.25.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r DyGLib/requirements.txt (line 3)) (2.0.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r DyGLib/requirements.txt (line 4)) (4.66.4)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from -r DyGLib/requirements.txt (line 5)) (0.9.0)\n","Collecting torch-geometric (from -r DyGLib/requirements.txt (line 6))\n","  Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r DyGLib/requirements.txt (line 1)) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r DyGLib/requirements.txt (line 1)) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r DyGLib/requirements.txt (line 1)) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r DyGLib/requirements.txt (line 1)) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r DyGLib/requirements.txt (line 1)) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r DyGLib/requirements.txt (line 1)) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.8.1->-r DyGLib/requirements.txt (line 1))\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.8.1->-r DyGLib/requirements.txt (line 1))\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.8.1->-r DyGLib/requirements.txt (line 1))\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.8.1->-r DyGLib/requirements.txt (line 1))\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.8.1->-r DyGLib/requirements.txt (line 1))\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.8.1->-r DyGLib/requirements.txt (line 1))\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.8.1->-r DyGLib/requirements.txt (line 1))\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.8.1->-r DyGLib/requirements.txt (line 1))\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.8.1->-r DyGLib/requirements.txt (line 1))\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.8.1->-r DyGLib/requirements.txt (line 1))\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.8.1->-r DyGLib/requirements.txt (line 1))\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r DyGLib/requirements.txt (line 1)) (2.3.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.1->-r DyGLib/requirements.txt (line 1))\n","  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->-r DyGLib/requirements.txt (line 3)) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r DyGLib/requirements.txt (line 3)) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r DyGLib/requirements.txt (line 3)) (2024.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric->-r DyGLib/requirements.txt (line 6)) (1.11.4)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric->-r DyGLib/requirements.txt (line 6)) (3.9.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric->-r DyGLib/requirements.txt (line 6)) (2.31.0)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric->-r DyGLib/requirements.txt (line 6)) (3.1.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric->-r DyGLib/requirements.txt (line 6)) (1.2.2)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric->-r DyGLib/requirements.txt (line 6)) (5.9.5)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->-r DyGLib/requirements.txt (line 3)) (1.16.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric->-r DyGLib/requirements.txt (line 6)) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric->-r DyGLib/requirements.txt (line 6)) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric->-r DyGLib/requirements.txt (line 6)) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric->-r DyGLib/requirements.txt (line 6)) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric->-r DyGLib/requirements.txt (line 6)) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric->-r DyGLib/requirements.txt (line 6)) (4.0.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->-r DyGLib/requirements.txt (line 1)) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric->-r DyGLib/requirements.txt (line 6)) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric->-r DyGLib/requirements.txt (line 6)) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric->-r DyGLib/requirements.txt (line 6)) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric->-r DyGLib/requirements.txt (line 6)) (2024.2.2)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric->-r DyGLib/requirements.txt (line 6)) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric->-r DyGLib/requirements.txt (line 6)) (3.5.0)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->-r DyGLib/requirements.txt (line 1)) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, torch-geometric, nvidia-cusolver-cu12\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 torch-geometric-2.5.3\n"]}],"source":["!pip install wget\n","\n","import os\n","import shutil\n","import wget\n","from urllib.parse import urlparse\n","\n","%cd /content/\n","\n","repo_path = \"https://github.com/KyawYeThu-11/DyGLib.git\"\n","repo_name = os.path.splitext(os.path.basename(urlparse(repo_path).path))[0]\n","\n","if not os.path.exists(repo_name):\n","  !git clone $repo_path\n","  !pip install -r DyGLib/requirements.txt"]},{"cell_type":"code","source":["# !python --version\n","# !pip install git+https://github.com/PyTorchLightning/pytorch-lightning"],"metadata":{"id":"Qj8FiYhUkaCH","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"u4Qx50hKysbZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716733520758,"user_tz":-540,"elapsed":15,"user":{"displayName":"Kyaw Ye Thu","userId":"03677166176861607126"}},"outputId":"785856ef-b541-4af5-9911-ef640564a090"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/DyGLib\n"]}],"source":["%cd /content/DyGLib"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"VYknFecnBLzG"},"outputs":[],"source":["import logging\n","import time\n","import sys\n","import os\n","from tqdm import tqdm\n","import numpy as np\n","import pandas as pd\n","import warnings\n","import shutil\n","import json\n","import torch\n","import torch.nn as nn\n","from torch_geometric.nn import global_mean_pool\n","# import lightning as L\n","from models.DyGFormerGraph import DyGFormerGraph\n","from models.DyGFormer import DyGFormer\n","from models.modules import MergeLayer, GraphRegressor\n","from utils.utils import set_random_seed, convert_to_gpu, get_parameter_sizes, create_optimizer\n","from utils.utils import get_neighbor_sampler, NegativeEdgeSampler\n","from utils.metrics import get_link_prediction_metrics\n","from utils.DataLoader import get_idx_data_loader, get_graph_regression_data\n","from utils.EarlyStopping import EarlyStopping\n","from utils.load_configs import get_link_prediction_args"]},{"cell_type":"markdown","metadata":{"id":"URXZEgpZvhwg"},"source":["# Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hN_4YQGW4Hyd"},"outputs":[],"source":["class Args:\n","    def __init__(self, dataset_name):\n","        self.dataset_name = dataset_name\n","        self.batch_size = 1000\n","        self.model_name = 'DyGFormer'\n","        self.gpu = 0\n","        self.num_neighbors = 20\n","        self.sample_neighbor_strategy = 'recent'\n","        self.time_scaling_factor = 1e-6\n","        self.num_walk_heads = 8\n","        self.num_folds = 5\n","        self.num_heads = 2\n","        self.num_layers = 2\n","        self.walk_length = 1\n","        self.time_gap = 2000\n","        self.time_feat_dim = 100\n","        self.position_feat_dim = 172\n","        self.edge_bank_memory_mode = 'unlimited_memory'\n","        self.time_window_mode = 'fixed_proportion'\n","        self.patch_size = 2\n","        self.channel_embedding_dim = 50\n","        self.max_input_sequence_length = 64\n","        self.learning_rate = 0.005\n","        self.load_checkpoint = True\n","        self.dropout = 0.05\n","        self.scheduler_period = 2\n","        self.num_epochs = 10\n","        self.optimizer = 'Adam'\n","        self.weight_decay = 0.0\n","        self.patience = 20\n","        self.test_interval_epochs = 10\n","        self.negative_sample_strategy = 'random'\n","        self.load_best_configs = False\n","\n","    def __str__(self):\n","        properties = [f\"{key}={value}\" for key, value in self.__dict__.items()]\n","        return f\"Args({', '.join(properties)})\"\n","\n","    def __repr__(self):\n","        return self.__str__()\n","\n","# Create an instance of Args with the loaded configuration\n","args = Args(dataset_name = 'connectome')\n","args.device = f'cuda:{args.gpu}' if torch.cuda.is_available() and args.gpu >= 0 else 'cpu'\n"]},{"cell_type":"markdown","metadata":{"id":"ouljAxbQJvZX"},"source":["### logger"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"515H-s4pJ7LJ"},"outputs":[],"source":["def set_up_logger(args):\n","        # set up logger\n","        logging.basicConfig(level=logging.INFO)\n","        logger = logging.getLogger()\n","        logger.setLevel(logging.DEBUG)\n","        os.makedirs(f\"./logs/{args.model_name}/{args.dataset_name}/\", exist_ok=True)\n","        # create file handler that logs debug and higher level messages\n","        fh = logging.FileHandler(f\"./logs/{args.model_name}/{args.dataset_name}/{str(time.time())}.log\")\n","        fh.setLevel(logging.DEBUG)\n","        # create console handler with a higher log level\n","        ch = logging.StreamHandler()\n","        ch.setLevel(logging.WARNING)\n","        # create formatter and add it to the handlers\n","        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n","        fh.setFormatter(formatter)\n","        ch.setFormatter(formatter)\n","        # add the handlers to logger\n","        logger.addHandler(fh)\n","        logger.addHandler(ch)\n","\n","        return logger, fh, ch"]},{"cell_type":"markdown","metadata":{"id":"3tAEIbw47HKM"},"source":["### main"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b0CV9WpdI9_E"},"outputs":[],"source":["def step_forward(dataset_dir, index, subject_id, label, loss_func):\n","  node_raw_features, edge_raw_features, full_data = \\\n","        get_graph_regression_data(dataset_dir=dataset_dir, index=index)\n","\n","  idx_data_loader = get_idx_data_loader(indices_list=list(range(len(full_data.src_node_ids))), batch_size=args.batch_size, shuffle=False)\n","  idx_data_loader_tqdm = tqdm(idx_data_loader, ncols=120)\n","\n","  # initialize validation and test neighbor sampler to retrieve temporal graph\n","  full_neighbor_sampler = get_neighbor_sampler(data=full_data, sample_neighbor_strategy=args.sample_neighbor_strategy,\n","                                                  time_scaling_factor=args.time_scaling_factor, seed=1)\n","  # training, only use training graph\n","  model[0].set_neighbor_sampler(full_neighbor_sampler)\n","\n","  graph_embedding_list = []\n","\n","  for batch_idx, train_data_indices in enumerate(idx_data_loader_tqdm):\n","\n","\n","    train_data_indices = train_data_indices.numpy()\n","    batch_src_node_ids, batch_dst_node_ids, batch_node_interact_times, batch_edge_ids = \\\n","                      full_data.src_node_ids[train_data_indices], full_data.dst_node_ids[train_data_indices], \\\n","                      full_data.node_interact_times[train_data_indices], full_data.edge_ids[train_data_indices]\n","\n","    graph_embedding = model[0](node_raw_features, edge_raw_features, batch_src_node_ids, batch_dst_node_ids, batch_node_interact_times)\n","    graph_embedding_list.append(graph_embedding)\n","\n","    idx_data_loader_tqdm.set_description(f'Epoch: {epoch + 1}, Subject: {subject_id}, training for the {batch_idx + 1}-th batch.')\n","\n","  graph_embeddings_tensor = torch.tensor(np.stack(graph_embedding_list).squeeze()).to(args.device)\n","\n","  predict = model[1](graph_embeddings_tensor)\n","  loss = loss_func(input=predict, target=label)\n","\n","  return predict, loss"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"GRR-DP0RKJj6","executionInfo":{"status":"error","timestamp":1716734494336,"user_tz":-540,"elapsed":485588,"user":{"displayName":"Kyaw Ye Thu","userId":"03677166176861607126"}},"outputId":"ab3ff933-a40b-4f0c-e8ae-52e4fa725e0d","collapsed":true},"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:root:configuration is Args(dataset_name=connectome, batch_size=1000, model_name=DyGFormer, gpu=0, num_neighbors=20, sample_neighbor_strategy=recent, time_scaling_factor=1e-06, num_walk_heads=8, num_folds=5, num_heads=2, num_layers=2, walk_length=1, time_gap=2000, time_feat_dim=100, position_feat_dim=172, edge_bank_memory_mode=unlimited_memory, time_window_mode=fixed_proportion, patch_size=2, channel_embedding_dim=50, max_input_sequence_length=64, learning_rate=0.005, load_checkpoint=True, dropout=0.05, scheduler_period=2, num_epochs=10, optimizer=Adam, weight_decay=0.0, patience=20, test_interval_epochs=10, negative_sample_strategy=random, load_best_configs=False, device=cpu)\n","INFO:root:model -> Sequential(\n","  (0): DyGFormerGraph(\n","    (time_encoder): TimeEncoder(\n","      (w): Linear(in_features=1, out_features=100, bias=True)\n","    )\n","    (neighbor_co_occurrence_encoder): NeighborCooccurrenceEncoder(\n","      (neighbor_co_occurrence_encode_layer): Sequential(\n","        (0): Linear(in_features=1, out_features=50, bias=True)\n","        (1): ReLU()\n","        (2): Linear(in_features=50, out_features=50, bias=True)\n","      )\n","    )\n","    (projection_layer): ModuleDict(\n","      (node): Linear(in_features=344, out_features=50, bias=True)\n","      (edge): Linear(in_features=344, out_features=50, bias=True)\n","      (time): Linear(in_features=200, out_features=50, bias=True)\n","      (neighbor_co_occurrence): Linear(in_features=100, out_features=50, bias=True)\n","    )\n","    (transformers): ModuleList(\n","      (0-1): 2 x TransformerEncoder(\n","        (multi_head_attention): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n","        )\n","        (dropout): Dropout(p=0.05, inplace=False)\n","        (linear_layers): ModuleList(\n","          (0): Linear(in_features=200, out_features=800, bias=True)\n","          (1): Linear(in_features=800, out_features=200, bias=True)\n","        )\n","        (norm_layers): ModuleList(\n","          (0-1): 2 x LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","    )\n","    (output_layer): Linear(in_features=200, out_features=172, bias=True)\n","  )\n","  (1): GraphRegressor(\n","    (conv1): Conv1d(172, 86, kernel_size=(3,), stride=(1,), padding=(1,))\n","    (conv2): Conv1d(86, 43, kernel_size=(3,), stride=(1,), padding=(1,))\n","    (fc1): Linear(in_features=43, out_features=1, bias=True)\n","    (act): ReLU()\n","  )\n",")\n","INFO:root:model name: DyGFormer, #parameters: 4431460 B, 4327.59765625 KB, 4.226169586181641 MB.\n","Epoch: 1, Subject: 103111, training for the 70-th batch.:  18%|████▏                   | 70/400 [08:03<38:01,  6.91s/it]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-092de122e3d6>\u001b[0m in \u001b[0;36m<cell line: 54>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# process every batch for a single subject and compute predictions and loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubject_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-9d6ce9ceb76b>\u001b[0m in \u001b[0;36mstep_forward\u001b[0;34m(dataset_dir, index, subject_id, label, loss_func)\u001b[0m\n\u001b[1;32m     22\u001b[0m                       \u001b[0mfull_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_interact_times\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_data_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_data_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mgraph_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_raw_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_raw_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_src_node_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dst_node_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_node_interact_times\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mgraph_embedding_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/DyGLib/models/DyGFormerGraph.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, node_raw_features, edge_raw_features, src_node_ids, dst_node_ids, node_interact_times)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_raw_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_raw_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0msrc_node_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_node_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_src_dst_node_temporal_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_node_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_node_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_node_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdst_node_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_interact_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_interact_times\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mmean_node_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msrc_node_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_node_embeddings\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/DyGLib/models/DyGFormerGraph.py\u001b[0m in \u001b[0;36mcompute_src_dst_node_temporal_embeddings\u001b[0;34m(self, src_node_ids, dst_node_ids, node_interact_times)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;31m# Tensor, shape (batch_size, src_num_patches + dst_num_patches, num_channels * channel_embedding_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtransformer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0mpatches_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatches_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# H^t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;31m# src_patches_data, Tensor, shape (batch_size, src_num_patches, num_channels * channel_embedding_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/DyGLib/models/DyGFormerGraph.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0;31m# Tensor, shape (batch_size, num_patches, self.attention_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m         \u001b[0;31m# Tensor, shape (batch_size, num_patches, self.attention_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["percentile_folder = '5-percentile'\n","save_model_folder = f\"/content/drive/MyDrive/CS471 Project/saved_models/graph_regression/{args.dataset_name}\"\n","os.makedirs(save_model_folder, exist_ok=True)\n","checkpoint_path = os.path.join(save_model_folder, f'{args.model_name}.pth')\n","\n","logger, fh, ch = set_up_logger(args)\n","\n","tart_time = time.time()\n","\n","logger.info(f'configuration is {args}')\n","\n","# Initialize the model\n","dynamic_backbone = DyGFormerGraph(node_feat_dim=args.position_feat_dim, edge_feat_dim=args.position_feat_dim, time_feat_dim=args.time_feat_dim,\n","                                 channel_embedding_dim=args.channel_embedding_dim, patch_size=args.patch_size,\n","                                 num_layers=args.num_layers, num_heads=args.num_heads, dropout=args.dropout,\n","                                 max_input_sequence_length=args.max_input_sequence_length, device=args.device)\n","\n","graph_regressor = GraphRegressor(in_channels=args.position_feat_dim, hidden_channels=int(args.position_feat_dim/2), out_channels=int(args.position_feat_dim/4))\n","model = nn.Sequential(dynamic_backbone, graph_regressor)\n","\n","# log the model structure\n","logger.info(f'model -> {model}')\n","logger.info(f'model name: {args.model_name}, #parameters: {get_parameter_sizes(model) * 4} B, '\n","                    f'{get_parameter_sizes(model) * 4 / 1024} KB, {get_parameter_sizes(model) * 4 / 1024 / 1024} MB.')\n","\n","# Create the optimizer and scheduler with specified parameters\n","optimizer = create_optimizer(model=model, optimizer_name=args.optimizer, learning_rate=args.learning_rate, weight_decay=args.weight_decay)\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.scheduler_period)\n","\n","# Convert the model to GPU if available\n","model = convert_to_gpu(model, device=args.device)\n","\n","\n","epoch_resumed = 0\n","index_resumed = 0\n","# Load checkpoint if specified and exists\n","if args.load_checkpoint == True and os.path.exists(checkpoint_path):\n","    checkpoint = torch.load(checkpoint_path, map_location=args.device)\n","    logger.info(f\"load model {checkpoint_path}\")\n","\n","    model.load_state_dict(checkpoint['model'])\n","    optimizer.load_state_dict(checkpoint['optimizer'])\n","    scheduler.load_state_dict(checkpoint['scheduler'])\n","\n","    epoch_resumed = checkpoint['epoch']\n","    index_resumed = checkpoint['subject_index'] + 1\n","    print(f\"Epoch resumed: {epoch_resumed}\")\n","    print(f\"Index resumed: {index_resumed}\")\n","\n","# Define the mean square loss function\n","loss_func = nn.MSELoss()\n","\n","# Training loop\n","for epoch in range(epoch_resumed, args.num_epochs, 1):\n","\n","    # Training for an epoch starts\n","    model.train()\n","\n","    train_losses = []\n","\n","    language_score_df = pd.read_csv(f'/content/drive/MyDrive/CS471 Project/{percentile_folder}-data/Train_Data_csv/Language_Task_Acc.csv')\n","\n","    # Iterate over each subject in the training data\n","    for index, row in language_score_df.iterrows():\n","        if index < index_resumed:\n","          continue\n","        subject_id = int(row.iloc[2])\n","        score = torch.Tensor([row.iloc[3]]).to(args.device)\n","        subject_folder = os.path.join(f'/content/drive/MyDrive/CS471 Project/Preprocessed Data/{percentile_folder}/Train', str(index))\n","\n","        # process every batch for a single subject and compute predictions and loss\n","        predict, loss = step_forward(subject_folder, index, subject_id, score, loss_func)\n","\n","        train_losses.append(loss.item())\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","\n","        print(f\"For subject {subject_id}, label: {score.item():.4f}, predict: {predict.item():.4f}, and loss: {loss:.4f}\")\n","\n","        # Save the checkpoint\n","        torch.save({\n","                  'subject_index': index,\n","                  'epoch': epoch,\n","                  'model': model.state_dict(),\n","                  'optimizer': optimizer.state_dict(),\n","                  'scheduler': scheduler.state_dict()},\n","                  checkpoint_path)\n","\n","    # validation\n","    print(\"---------------Validation Starts--------------\")\n","\n","    model.eval()\n","\n","    val_losses = []\n","\n","    with torch.no_grad():\n","        language_score_df = pd.read_csv(f'/content/drive/MyDrive/CS471 Project/{percentile_folder}-data/Val_Data_csv/Language_Task_Acc.csv')\n","\n","        for index, row in language_score_df.iterrows():\n","            subject_id = int(row.iloc[2])\n","            score = torch.Tensor([row.iloc[3]]).to(args.device)\n","            subject_folder = os.path.join(f'/content/drive/MyDrive/CS471 Project/Preprocessed Data/{percentile_folder}/Val', str(index))\n","\n","            predict, loss = step_forward(subject_folder, subject_id, score, loss_func)\n","\n","            val_losses.append(loss.item())\n","\n","            print(f\"For subject {subject_id}, label: {score.item():.4f}, predict: {predict.item():.4f}, and loss: {loss:.4f}\")\n","\n","    print(\"-----------------------------------\")\n","    logger.info(f'Epoch: {epoch}, learning rate: {optimizer.param_groups[0][\"lr\"]}, train loss: {np.mean(train_losses):.4f} and val loss: {np.mean(val_losses):.4f}')\n","    print(\"-----------------------------------\")\n","\n","    # Save the results of the current run to a JSON file\n","    result_json = {\n","            \"train losses\": [f'{loss:.4f}' for loss in train_losses],\n","            'val losses': [f'{loss:.4f}' for loss in val_losses]\n","    }\n","\n","    result_json = json.dumps(result_json, indent=4)\n","\n","    save_result_folder = f\"/content/drive/MyDrive/CS471 Project/saved_results/graph_regression/{args.dataset_name}\"\n","    os.makedirs(save_result_folder, exist_ok=True)\n","    save_result_path = os.path.join(save_result_folder, f\"{args.save_model_name}.json\")\n","\n","    with open(save_result_path, 'w') as file:\n","        file.write(result_json)\n","\n","torch.save(model.state_dict(), os.path.join(save_model_folder, f'{args.model_name}_final.pth'))"]},{"cell_type":"markdown","source":["## Testing"],"metadata":{"id":"vFsUY6EmS7FK"}},{"cell_type":"markdown","source":["Load the model."],"metadata":{"id":"mlwy-bdVTLBT"}},{"cell_type":"code","source":["# Initialize the model\n","dynamic_backbone = DyGFormerGraph(node_feat_dim=args.position_feat_dim, edge_feat_dim=args.position_feat_dim, time_feat_dim=args.time_feat_dim,\n","                                 channel_embedding_dim=args.channel_embedding_dim, patch_size=args.patch_size,\n","                                 num_layers=args.num_layers, num_heads=args.num_heads, dropout=args.dropout,\n","                                 max_input_sequence_length=args.max_input_sequence_length, device=args.device)\n","\n","graph_regressor = GraphRegressor(in_channels=args.position_feat_dim, hidden_channels=int(args.position_feat_dim/2), out_channels=int(args.position_feat_dim/4))\n","model = nn.Sequential(dynamic_backbone, graph_regressor)\n","\n","# Load the pretrained model from the specified path\n","load_model_path = f\"/content/drive/MyDrive/CS471 Project/saved_models/graph_regression/DyGFormer.pth\"\n","checkpoint = torch.load(load_model_path, map_location=args.device)\n","model.load_state_dict(checkpoint['model_state_dict'])"],"metadata":{"id":"Rk9ZgOgWTDnS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Test the model with the testing set."],"metadata":{"id":"mGX3fGf4TNpb"}},{"cell_type":"code","source":["test_losses = []\n","\n","with torch.no_grad():\n","    language_score_df = pd.read_csv(f'/content/drive/MyDrive/CS471 Project/{percentile_folder}-data/Test_Data_csv/Language_Task_Acc.csv')\n","\n","    for index, row in language_score_df.iterrows():\n","        subject_id = int(row.iloc[2])\n","        score = torch.Tensor([row.iloc[3]]).to(args.device)\n","        subject_folder = os.path.join(f'/content/drive/MyDrive/CS471 Project/Preprocessed Data/{percentile_folder}/Test', str(index))\n","\n","        predict, loss = step_forward(subject_folder, subject_id, score, loss_func)\n","\n","        test_losses.append(loss.item())\n","\n","        print(f\"For subject {subject_id}, label: {score.item():.4f}, predict: {predict.item():.4f}, and loss: {loss:.4f}\")\n","\n","    print(f'test loss: {np.mean(val_losses):.4f}')"],"metadata":{"id":"C8iturmKTO7T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gFJOa1ru9WjM"},"source":["# Training Pipeline (v2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZOt4rb02o6Xn"},"outputs":[],"source":["import pandas as pd\n","import cudf\n","import random"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mduUBdD1pAzE"},"outputs":[],"source":["# numBatch = 200\n","def csv2triplets(csvDf, dir, num):\n","  triplets=torch.tensor(cudf.DataFrame(csvDf, columns=['source', 'dest', 'time_interval']).values)\n","  triplets=triplets.reshape(50, 120, 200, 3)\n","  torch.save(triplets, os.path.join(dir, 'triplets'+str(num)+'.pt'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fnb12CHUJuZg"},"outputs":[],"source":["\n","num_folds=5\n","for k in range(num_folds):\n","  print(f\"Fold {k+1}\")\n","  for i in range(40):\n","    print(f\"Iteration {i}\")\n","    csvDf=cudf.read_csv(os.path.join('./DG_data/Train/Fold'+str(k), 'train_'+str(i)+'.csv'))\n","    if(csvDf.shape[0]!=50*120*200):\n","      randomDf=cudf.DataFrame(columns=['source', 'dest', 'time_interval', 'edge_label', 'w'])\n","      dic={'source':[random.randint(0, 399) for j in range(50*120*200-csvDf.shape[0])],\n","           'dest':[random.randint(0, 399) for j in range(50*120*200-csvDf.shape[0])],\n","           'time_interval':[random.randint(0, 49) for j in range(50*120*200-csvDf.shape[0])],\n","           'edge_label':[0 for j in range(50*120*200-csvDf.shape[0])],\n","           'w':[0 for j in range(50*120*200-csvDf.shape[0])]}\n","      randomDf=cudf.DataFrame(dic)\n","      csvDf=cudf.concat([csvDf, randomDf], axis=0, ignore_index=True)\n","      csvDf=csvDf.iloc[:,[0,1,2]]\n","      print(csvDf.shape[0])\n","    assert csvDf.shape[0]==50*120*200\n","    csvDf.sort_values(by=['time_interval', 'source', 'dest'])\n","    csv2triplets(csvDf,  './DG_data/Train/Fold'+str(k), i)\n","  for i in range(10):\n","    csvDf=cudf.read_csv(os.path.join('./DG_data/Test/Fold'+str(k), 'test_'+str(i)+'.csv'))\n","    if(csvDf.shape[0]!=50*120*200):\n","      randomDf=cudf.DataFrame(columns=['source', 'dest', 'time_interval', 'edge_label', 'w'])\n","      dic={'source':[random.randint(0, 399) for j in range(50*120*200-csvDf.shape[0])],\n","           'dest':[random.randint(0, 399) for j in range(50*120*200-csvDf.shape[0])],\n","           'time_interval':[random.randint(0, 49) for j in range(50*120*200-csvDf.shape[0])],\n","           'edge_label':[0 for j in range(50*120*200-csvDf.shape[0])],\n","           'w':[0 for j in range(50*120*200-csvDf.shape[0])]}\n","      randomDf=cudf.DataFrame(dic)\n","      csvDf=cudf.concat([csvDf, randomDf], axis=0, ignore_index=True)\n","      csvDf=csvDf.iloc[:,[0,1,2]]\n","    csvDf.sort_values(by=['time_interval', 'source', 'dest'])\n","    csv2triplets(csvDf, './DG_data/Test/Fold'+str(k), i)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4UikrU2QAZ_b"},"outputs":[],"source":["class MLPRegressor(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim):\n","        super(MLPRegressor, self).__init__()\n","        self.fc1 = nn.Linear(input_dim, hidden_dim)\n","        self.relu=nn.ReLU()\n","        self.fc2 = nn.Linear(hidden_dim, output_dim)\n","    def forward(self, x):\n","        out = self.fc1(x)\n","        out = self.relu(out)\n","        out = self.fc2(out)\n","        return out"]},{"cell_type":"code","source":["class NeighborSampler:\n","\n","    def __init__(self, adj_list: list, sample_neighbor_strategy: str = 'uniform', time_scaling_factor: float = 0.0, seed: int = None):\n","        \"\"\"\n","        Neighbor sampler.\n","        :param adj_list: list, list of list, where each element is a list of triple tuple (node_id, edge_id, timestamp)\n","        :param sample_neighbor_strategy: str, how to sample historical neighbors, 'uniform', 'recent', or 'time_interval_aware'\n","        :param time_scaling_factor: float, a hyper-parameter that controls the sampling preference with time interval,\n","        a large time_scaling_factor tends to sample more on recent links, this parameter works when sample_neighbor_strategy == 'time_interval_aware'\n","        :param seed: int, random seed\n","        \"\"\"\n","        self.sample_neighbor_strategy = sample_neighbor_strategy\n","        self.seed = seed\n","\n","        # list of each node's neighbor ids, edge ids and interaction times, which are sorted by interaction times\n","        self.nodes_neighbor_ids = []\n","        self.nodes_edge_ids = []\n","        self.nodes_neighbor_times = []\n","\n","        if self.sample_neighbor_strategy == 'time_interval_aware':\n","            self.nodes_neighbor_sampled_probabilities = []\n","            self.time_scaling_factor = time_scaling_factor\n","\n","        # the list at the first position in adj_list is empty, hence, sorted() will return an empty list for the first position\n","        # its corresponding value in self.nodes_neighbor_ids, self.nodes_edge_ids, self.nodes_neighbor_times will also be empty with length 0\n","        for node_idx, per_node_neighbors in enumerate(adj_list):\n","            # per_node_neighbors is a list of tuples (neighbor_id, edge_id, timestamp)\n","            # sort the list based on timestamps, sorted() function is stable\n","            # Note that sort the list based on edge id is also correct, as the original data file ensures the interactions are chronological\n","            sorted_per_node_neighbors = sorted(per_node_neighbors, key=lambda x: x[2])\n","            self.nodes_neighbor_ids.append(np.array([x[0] for x in sorted_per_node_neighbors]))\n","            self.nodes_edge_ids.append(np.array([x[1] for x in sorted_per_node_neighbors]))\n","            self.nodes_neighbor_times.append(np.array([x[2] for x in sorted_per_node_neighbors]))\n","\n","            # additional for time interval aware sampling strategy (proposed in CAWN paper)\n","            if self.sample_neighbor_strategy == 'time_interval_aware':\n","                self.nodes_neighbor_sampled_probabilities.append(self.compute_sampled_probabilities(np.array([x[2] for x in sorted_per_node_neighbors])))\n","\n","        if self.seed is not None:\n","            self.random_state = np.random.RandomState(self.seed)\n","\n","    def compute_sampled_probabilities(self, node_neighbor_times: np.ndarray):\n","        \"\"\"\n","        compute the sampled probabilities of historical neighbors based on their interaction times\n","        :param node_neighbor_times: ndarray, shape (num_historical_neighbors, )\n","        :return:\n","        \"\"\"\n","        if len(node_neighbor_times) == 0:\n","            return np.array([])\n","        # compute the time delta with regard to the last time in node_neighbor_times\n","        node_neighbor_times = node_neighbor_times - np.max(node_neighbor_times)\n","        # compute the normalized sampled probabilities of historical neighbors\n","        exp_node_neighbor_times = np.exp(self.time_scaling_factor * node_neighbor_times)\n","        sampled_probabilities = exp_node_neighbor_times / np.cumsum(exp_node_neighbor_times)\n","        # note that the first few values in exp_node_neighbor_times may be all zero, which make the corresponding values in sampled_probabilities\n","        # become nan (divided by zero), so we replace the nan by a very large negative number -1e10 to denote the sampled probabilities\n","        sampled_probabilities[np.isnan(sampled_probabilities)] = -1e10\n","        return sampled_probabilities\n","\n","    def find_neighbors_before(self, node_id: int, interact_time: float, return_sampled_probabilities: bool = False):\n","        \"\"\"\n","        extracts all the interactions happening before interact_time (less than interact_time) for node_id in the overall interaction graph\n","        the returned interactions are sorted by time.\n","        :param node_id: int, node id\n","        :param interact_time: float, interaction time\n","        :param return_sampled_probabilities: boolean, whether return the sampled probabilities of neighbors\n","        :return: neighbors, edge_ids, timestamps and sampled_probabilities (if return_sampled_probabilities is True) with shape (historical_nodes_num, )\n","        \"\"\"\n","        # return index i, which satisfies list[i - 1] < v <= list[i]\n","        # return 0 for the first position in self.nodes_neighbor_times since the value at the first position is empty\n","        i = np.searchsorted(self.nodes_neighbor_times[node_id], interact_time)\n","\n","        if return_sampled_probabilities:\n","            return self.nodes_neighbor_ids[node_id][:i], self.nodes_edge_ids[node_id][:i], self.nodes_neighbor_times[node_id][:i], \\\n","                   self.nodes_neighbor_sampled_probabilities[node_id][:i]\n","        else:\n","            return self.nodes_neighbor_ids[node_id][:i], self.nodes_edge_ids[node_id][:i], self.nodes_neighbor_times[node_id][:i], None\n","\n","    def get_historical_neighbors(self, node_ids: np.ndarray, node_interact_times: np.ndarray, num_neighbors: int = 20):\n","        \"\"\"\n","        get historical neighbors of nodes in node_ids with interactions before the corresponding time in node_interact_times\n","        :param node_ids: ndarray, shape (batch_size, ) or (*, ), node ids\n","        :param node_interact_times: ndarray, shape (batch_size, ) or (*, ), node interaction times\n","        :param num_neighbors: int, number of neighbors to sample for each node\n","        :return:\n","        \"\"\"\n","        assert num_neighbors > 0, 'Number of sampled neighbors for each node should be greater than 0!'\n","        # All interactions described in the following three matrices are sorted in each row by time\n","        # each entry in position (i,j) represents the id of the j-th dst node of src node node_ids[i] with an interaction before node_interact_times[i]\n","        # ndarray, shape (batch_size, num_neighbors)\n","        nodes_neighbor_ids = np.zeros((len(node_ids), num_neighbors)).astype(np.longlong)\n","        # each entry in position (i,j) represents the id of the edge with src node node_ids[i] and dst node nodes_neighbor_ids[i][j] with an interaction before node_interact_times[i]\n","        # ndarray, shape (batch_size, num_neighbors)\n","        nodes_edge_ids = np.zeros((len(node_ids), num_neighbors)).astype(np.longlong)\n","        # each entry in position (i,j) represents the interaction time between src node node_ids[i] and dst node nodes_neighbor_ids[i][j], before node_interact_times[i]\n","        # ndarray, shape (batch_size, num_neighbors)\n","        nodes_neighbor_times = np.zeros((len(node_ids), num_neighbors)).astype(np.float32)\n","\n","        # extracts all neighbors ids, edge ids and interaction times of nodes in node_ids, which happened before the corresponding time in node_interact_times\n","        for idx, (node_id, node_interact_time) in enumerate(zip(node_ids, node_interact_times)):\n","            # find neighbors that interacted with node_id before time node_interact_time\n","            node_neighbor_ids, node_edge_ids, node_neighbor_times, node_neighbor_sampled_probabilities = \\\n","                self.find_neighbors_before(node_id=node_id, interact_time=node_interact_time, return_sampled_probabilities=self.sample_neighbor_strategy == 'time_interval_aware')\n","\n","            if len(node_neighbor_ids) > 0:\n","                if self.sample_neighbor_strategy in ['uniform', 'time_interval_aware']:\n","                    # when self.sample_neighbor_strategy == 'uniform', we shuffle the data before sampling with node_neighbor_sampled_probabilities as None\n","                    # when self.sample_neighbor_strategy == 'time_interval_aware', we sample neighbors based on node_neighbor_sampled_probabilities\n","                    # for time_interval_aware sampling strategy, we additionally use softmax to make the sum of sampled probabilities be 1\n","                    if node_neighbor_sampled_probabilities is not None:\n","                        # for extreme case that node_neighbor_sampled_probabilities only contains -1e10, which will make the denominator of softmax be zero,\n","                        # torch.softmax() function can tackle this case\n","                        node_neighbor_sampled_probabilities = torch.softmax(torch.from_numpy(node_neighbor_sampled_probabilities).float(), dim=0).numpy()\n","                    if self.seed is None:\n","                        sampled_indices = np.random.choice(a=len(node_neighbor_ids), size=num_neighbors, p=node_neighbor_sampled_probabilities)\n","                    else:\n","                        sampled_indices = self.random_state.choice(a=len(node_neighbor_ids), size=num_neighbors, p=node_neighbor_sampled_probabilities)\n","\n","                    nodes_neighbor_ids[idx, :] = node_neighbor_ids[sampled_indices]\n","                    nodes_edge_ids[idx, :] = node_edge_ids[sampled_indices]\n","                    nodes_neighbor_times[idx, :] = node_neighbor_times[sampled_indices]\n","\n","                    # resort based on timestamps, return the ids in sorted increasing order, note this maybe unstable when multiple edges happen at the same time\n","                    # (we still do this though this is unnecessary for TGAT or CAWN to guarantee the order of nodes,\n","                    # since TGAT computes in an order-agnostic manner with relative time encoding, and CAWN computes for each walk while the sampled nodes are in different walks)\n","                    sorted_position = nodes_neighbor_times[idx, :].argsort()\n","                    nodes_neighbor_ids[idx, :] = nodes_neighbor_ids[idx, :][sorted_position]\n","                    nodes_edge_ids[idx, :] = nodes_edge_ids[idx, :][sorted_position]\n","                    nodes_neighbor_times[idx, :] = nodes_neighbor_times[idx, :][sorted_position]\n","                elif self.sample_neighbor_strategy == 'recent':\n","                    # Take most recent interactions with number num_neighbors\n","                    node_neighbor_ids = node_neighbor_ids[-num_neighbors:]\n","                    node_edge_ids = node_edge_ids[-num_neighbors:]\n","                    node_neighbor_times = node_neighbor_times[-num_neighbors:]\n","\n","                    # put the neighbors' information at the back positions\n","                    nodes_neighbor_ids[idx, num_neighbors - len(node_neighbor_ids):] = node_neighbor_ids\n","                    nodes_edge_ids[idx, num_neighbors - len(node_edge_ids):] = node_edge_ids\n","                    nodes_neighbor_times[idx, num_neighbors - len(node_neighbor_times):] = node_neighbor_times\n","                else:\n","                    raise ValueError(f'Not implemented error for sample_neighbor_strategy {self.sample_neighbor_strategy}!')\n","\n","        # three ndarrays, with shape (batch_size, num_neighbors)\n","        return nodes_neighbor_ids, nodes_edge_ids, nodes_neighbor_times\n","\n","    def get_multi_hop_neighbors(self, num_hops: int, node_ids: np.ndarray, node_interact_times: np.ndarray, num_neighbors: int = 20):\n","        \"\"\"\n","        get historical neighbors of nodes in node_ids within num_hops hops\n","        :param num_hops: int, number of sampled hops\n","        :param node_ids: ndarray, shape (batch_size, ), node ids\n","        :param node_interact_times: ndarray, shape (batch_size, ), node interaction times\n","        :param num_neighbors: int, number of neighbors to sample for each node\n","        :return:\n","        \"\"\"\n","        assert num_hops > 0, 'Number of sampled hops should be greater than 0!'\n","\n","        # get the temporal neighbors at the first hop\n","        # nodes_neighbor_ids, nodes_edge_ids, nodes_neighbor_times -> ndarray, shape (batch_size, num_neighbors)\n","        nodes_neighbor_ids, nodes_edge_ids, nodes_neighbor_times = self.get_historical_neighbors(node_ids=node_ids,\n","                                                                                                 node_interact_times=node_interact_times,\n","                                                                                                 num_neighbors=num_neighbors)\n","        # three lists to store the neighbor ids, edge ids and interaction timestamp information\n","        nodes_neighbor_ids_list = [nodes_neighbor_ids]\n","        nodes_edge_ids_list = [nodes_edge_ids]\n","        nodes_neighbor_times_list = [nodes_neighbor_times]\n","        for hop in range(1, num_hops):\n","            # get information of neighbors sampled at the current hop\n","            # three ndarrays, with shape (batch_size * num_neighbors ** hop, num_neighbors)\n","            nodes_neighbor_ids, nodes_edge_ids, nodes_neighbor_times = self.get_historical_neighbors(node_ids=nodes_neighbor_ids_list[-1].flatten(),\n","                                                                                                     node_interact_times=nodes_neighbor_times_list[-1].flatten(),\n","                                                                                                     num_neighbors=num_neighbors)\n","            # three ndarrays with shape (batch_size, num_neighbors ** (hop + 1))\n","            nodes_neighbor_ids = nodes_neighbor_ids.reshape(len(node_ids), -1)\n","            nodes_edge_ids = nodes_edge_ids.reshape(len(node_ids), -1)\n","            nodes_neighbor_times = nodes_neighbor_times.reshape(len(node_ids), -1)\n","\n","            nodes_neighbor_ids_list.append(nodes_neighbor_ids)\n","            nodes_edge_ids_list.append(nodes_edge_ids)\n","            nodes_neighbor_times_list.append(nodes_neighbor_times)\n","\n","        # tuple, each element in the tuple is a list of num_hops ndarrays, each with shape (batch_size, num_neighbors ** current_hop)\n","        return nodes_neighbor_ids_list, nodes_edge_ids_list, nodes_neighbor_times_list\n","\n","    def get_all_first_hop_neighbors(self, node_ids: np.ndarray, node_interact_times: np.ndarray):\n","        \"\"\"\n","        get historical neighbors of nodes in node_ids at the first hop with max_num_neighbors as the maximal number of neighbors (make the computation feasible)\n","        :param node_ids: ndarray, shape (batch_size, ), node ids\n","        :param node_interact_times: ndarray, shape (batch_size, ), node interaction times\n","        :return:\n","        \"\"\"\n","        # three lists to store the first-hop neighbor ids, edge ids and interaction timestamp information, with batch_size as the list length\n","        nodes_neighbor_ids_list, nodes_edge_ids_list, nodes_neighbor_times_list = [], [], []\n","        # get the temporal neighbors at the first hop\n","        for idx, (node_id, node_interact_time) in enumerate(zip(node_ids, node_interact_times)):\n","            # find neighbors that interacted with node_id before time node_interact_time\n","            node_neighbor_ids, node_edge_ids, node_neighbor_times, _ = self.find_neighbors_before(node_id=node_id,\n","                                                                                                  interact_time=node_interact_time,\n","                                                                                                  return_sampled_probabilities=False)\n","            nodes_neighbor_ids_list.append(node_neighbor_ids)\n","            nodes_edge_ids_list.append(node_edge_ids)\n","            nodes_neighbor_times_list.append(node_neighbor_times)\n","\n","        return nodes_neighbor_ids_list, nodes_edge_ids_list, nodes_neighbor_times_list\n","\n","    def reset_random_state(self):\n","        \"\"\"\n","        reset the random state by self.seed\n","        :return:\n","        \"\"\"\n","        self.random_state = np.random.RandomState(self.seed)\n","\n"],"metadata":{"id":"mRg4jIJaw3kW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_neighbor_sampler(data, sample_neighbor_strategy: str = 'uniform', time_scaling_factor: float = 0.0, seed: int = None):\n","    \"\"\"\n","    get neighbor sampler\n","    :param data: Data\n","    :param sample_neighbor_strategy: str, how to sample historical neighbors, 'uniform', 'recent', or 'time_interval_aware''\n","    :param time_scaling_factor: float, a hyper-parameter that controls the sampling preference with time interval,\n","    a large time_scaling_factor tends to sample more on recent links, this parameter works when sample_neighbor_strategy == 'time_interval_aware'\n","    :param seed: int, random seed\n","    :return:\n","    \"\"\"\n","    max_node_id = max(data[:,0].max(), data[:,1].max())\n","    # the adjacency vector stores edges for each node (source or destination), undirected\n","    # adj_list, list of list, where each element is a list of triple tuple (node_id, edge_id, timestamp)\n","    # the list at the first position in adj_list is empty\n","    adj_list = [[] for _ in range(max_node_id + 1)]\n","    edge_ids=torch.as_tensor([0 for i in range(data.shape[0])])\n","    for src_node_id, dst_node_id, edge_id, node_interact_time in zip(data[:,0], data[:,1], edge_ids, data[2]):\n","        adj_list[src_node_id].append((dst_node_id, edge_id, node_interact_time))\n","        adj_list[dst_node_id].append((src_node_id, edge_id, node_interact_time))\n","\n","    return NeighborSampler(adj_list=adj_list, sample_neighbor_strategy=sample_neighbor_strategy, time_scaling_factor=time_scaling_factor, seed=seed)\n"],"metadata":{"id":"va8nwcB6w8VQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OqYQxRkk7KS-"},"outputs":[],"source":["if os.path.isfile(os.path.join('./drive/MyDrive/CS471 Project', 'checkpoint.pth')):\n","        print('resuming checkpoint experiment')\n","        checkpoint = torch.load(os.path.join('./drive/MyDrive/CS471 Project', 'checkpoint.pth'), map_location='cuda')\n","else:\n","  checkpoint = {\n","    'fold': 0,\n","    'epoch': 0,\n","    'subject': 0,\n","    'model': None,\n","    'mode': 'sum',\n","    'optimizer': None,\n","    'scheduler': None}\n","\n","num_folds=5\n","num_epochs=100\n","num_subjects=40\n","num_timepoints=50\n","node_dim=172\n","edge_dim=172\n","learning_rate=0.001\n","bestLoss=np.zeros(5)\n","for k_index in range(num_folds):\n","  if checkpoint['fold']:\n","    if k_index < checkpoint['fold']:\n","      continue\n","  model=DyGFormerGraph(node_feat_dim=node_dim, edge_feat_dim=edge_dim, time_feat_dim=args.time_feat_dim,\n","                                 channel_embedding_dim=args.channel_embedding_dim, device='cuda')\n","  model.node_raw_features=torch.zeros((400+1,172)).to('cuda')\n","  model.edge_raw_features=torch.zeros((400+1,172)).to('cuda')\n","\n","\n","  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n","  gRegressor=GraphRegressor(in_channels=1, hidden_channels=2, out_channels=1).half()\n","  gRegressor.to('cuda')\n","  mRegressor=MLPRegressor(input_dim=172, hidden_dim=13, output_dim=1).half()\n","  mRegressor.to('cuda')\n","  criterion=torch.nn.MSELoss()\n","  minLoss=np.Inf\n","  tolerance=0\n","  threshold=5\n","\n","  if checkpoint['model'] is not None: model.load_state_dict(checkpoint['model'])\n","  if checkpoint['optimizer'] is not None: optimizer.load_state_dict(checkpoint['optimizer'])\n","  if checkpoint['scheduler'] is not None: scheduler.load_state_dict(checkpoint['scheduler'])\n","  for epoch in range(checkpoint['epoch'], num_epochs):\n","    embs=torch.zeros((node_dim,num_subjects))\n","    for i in range(checkpoint['subject'], num_subjects):\n","      triplets=torch.load(f'./DG_data/Train/Fold{k}/triplets{i}.pt')\n","      assert triplets.shape==(num_timepoints, 120, 200, 3)\n","      triplets3=triplets.reshape(-1,3)\n","\n","      scoresDf=cudf.read_csv(f'./DG_data/Train/Fold{k}/Language_Task_Acc.csv')\n","      scoresDf=scoresDf.iloc[:,[1,3]]\n","      scores=torch.as_tensor(scoresDf.values).to('cuda')\n","\n","      dataloader=torch.utils.data.DataLoader(triplets, batch_size=1, shuffle=False, num_workers=4, pin_memory=True)\n","      graphEmbs=torch.zeros((node_dim, num_timepoints), device=torch.device('cuda'))\n","\n","      for t, data in enumerate(tqdm(dataloader)):\n","        data=data.squeeze(0)\n","        acrossB=torch.zeros((node_dim, 120)).to('cuda')\n","        for b, batch in enumerate(data):\n","          batch=batch.detach().to('cpu').numpy()\n","          srcNodes=batch[:,0]\n","          dstNodes=batch[:,1]\n","          timepoints=batch[:,2]\n","          train_neighbor_sampler = get_neighbor_sampler(data=batch, sample_neighbor_strategy=args.sample_neighbor_strategy,\n","                                                  time_scaling_factor=args.time_scaling_factor, seed=0)\n","          model.set_neighbor_sampler(train_neighbor_sampler)\n","          model.to('cuda')\n","          with torch.no_grad():\n","            batch_src_node_embeddings, batch_dst_node_embeddings = \\\n","            model.compute_src_dst_node_temporal_embeddings(src_node_ids=srcNodes, dst_node_ids=dstNodes, node_interact_times=timepoints)\n","            if checkpoint['mode']=='sum':\n","              batch_graph_embeddings=torch.sum(torch.cat([batch_src_node_embeddings, batch_dst_node_embeddings], dim=0), dim=0)\n","            if checkpoint['mode']=='mean':\n","              batch_graph_embeddings=torch.mean(torch.cat([batch_src_node_embeddings, batch_dst_node_embeddings], dim=0), dim=0)\n","            batch_graph_embeddings=batch_graph_embeddings.to('cuda').t()\n","          acrossB[:,b]=batch_graph_embeddings\n","          del train_neighbor_sampler\n","          del batch\n","          del srcNodes\n","          del dstNodes\n","          del timepoints\n","          del batch_src_node_embeddings\n","          del batch_dst_node_embeddings\n","          del batch_graph_embeddings\n","          torch.cuda.empty_cache()\n","        bMean=torch.mean(acrossB, dim=1)\n","        graphEmbs[:,t]=bMean\n","        del data\n","        del acrossB\n","        del bMean\n","        torch.cuda.empty_cache()\n","      graphEmbs=graphEmbs.unsqueeze(1).permute(1,0,2)\n","      #graphEmbs.shape==(172,1,50)\n","\n","      regEmb=gRegressor(graphEmbs).squeeze(1)\n","      print(regEmb.shape)\n","      embs[:,i]=regEmb\n","      del regEmb\n","      del graphEmbs\n","\n","    embs=embs.t()\n","    embsMLP=mRegressor(embs)\n","\n","    scores=scores.reshape(-1,1)\n","    optimizer.zero_grad()\n","    loss=criterion(embsMLP, scores)\n","    loss.backward()\n","    optimizer.step()\n","    scheduler.step()\n","\n","    print(f'Training - k:{k} e:{epoch} loss:{loss}')\n","    if loss<minLoss:\n","      minLoss=loss\n","      tolerance=0\n","    else:\n","      tolerance=tolerance+1\n","\n","\n","\n","    torch.save({\n","                'fold': k,\n","                'epoch': epoch+1,\n","                'model': model.state_dict(),\n","                'mode': checkpoint['mode'],\n","                'optimizer': optimizer.state_dict(),\n","                'scheduler': scheduler.state_dict()},\n","                os.path.join('./drive/MyDrive/CS471 Project', 'checkpoint.pth'))\n","\n","    if tolerance==threshold:\n","      break\n","\n","\n","\n","  torch.save(model.state_dict(), os.path.join('./drive/MyDrive/CS471 Project', f'model{k}.pth'))\n","  checkpoint.update({'epoch': 0, 'model': None, 'optimizer': None, 'scheduler': None})\n","  bestLoss[i]=minLoss\n","\n","pd.DataFrame(bestLoss).to_csv('./drive/MyDrive/CS471 Project/bestLoss.csv')"]},{"cell_type":"code","source":["class DyGFormerGraphL(L.LightningModule):\n","  def __init__(self,node_dim, edge_dim, time_feat_dim, channel_embedding_dim):\n","    super().__init__()\n","    self.model=DyGFormerGraph(node_feat_dim=node_dim, edge_feat_dim=edge_dim, time_feat_dim=time_feat_dim,\n","                                 channel_embedding_dim=channel_embedding_dim)\n","    self.model.node_raw_features=torch.zeros((400+1,node_dim))\n","    self.model.edge_raw_features=torch.zeros((400+1,edge_dim))\n","  def forward(self, input4d):\n","    for t, data in enumerate(tqdm(input4d)):\n","      data=data.squeeze(0)\n","      acrossB=torch.zeros((node_dim, 120))\n","      for b, batch in enumerate(data):\n","        batch=batch.detach().numpy()\n","        srcNodes=batch[:,0]\n","        dstNodes=batch[:,1]\n","        timepoints=batch[:,2]\n","        train_neighbor_sampler = get_neighbor_sampler(data=batch, sample_neighbor_strategy=args.sample_neighbor_strategy,\n","                                                time_scaling_factor=args.time_scaling_factor, seed=0)\n","        self.model.set_neighbor_sampler(train_neighbor_sampler)\n","        with torch.no_grad():\n","          batch_src_node_embeddings, batch_dst_node_embeddings = \\\n","          self.model.compute_src_dst_node_temporal_embeddings(src_node_ids=srcNodes, dst_node_ids=dstNodes, node_interact_times=timepoints)\n","          if checkpoint['mode']=='sum':\n","            batch_graph_embeddings=torch.sum(torch.cat([batch_src_node_embeddings, batch_dst_node_embeddings], dim=0), dim=0)\n","          if checkpoint['mode']=='mean':\n","            batch_graph_embeddings=torch.mean(torch.cat([batch_src_node_embeddings, batch_dst_node_embeddings], dim=0), dim=0)\n","          batch_graph_embeddings=batch_graph_embeddings.t()\n","        acrossB[:,b]=batch_graph_embeddings\n","\n","      bMean=torch.mean(acrossB, dim=1)\n","      graphEmbs[:,t]=bMean\n","    graphEmbs=graphEmbs.unsqueeze(1).permute(1,0,2)\n","    #graphEmbs.shape==(172,1,50)\n","\n","    return graphEmbs\n","\n","  def training_step():\n","    gRegressor=GraphRegressor(in_channels=1, hidden_channels=2, out_channels=1)\n","    mRegressor=MLPRegressor(input_dim=172, hidden_dim=13, output_dim=1)\n","    criterion=torch.nn.MSELoss()\n","    minLoss=np.Inf\n","    tolerance=0\n","    threshold=5\n","\n","    triplets=torch.load(f'./DG_data/Train/Fold{k}/triplets{i}.pt')\n","    assert triplets.shape==(num_timepoints, 120, 200, 3)\n","    triplets3=triplets.reshape(-1,3)\n","\n","    scoresDf=cudf.read_csv(f'./DG_data/Train/Fold{k}/Language_Task_Acc.csv')\n","    scoresDf=scoresDf.iloc[:,[1,3]]\n","    scores=torch.as_tensor(scoresDf.values).to('cuda')\n","\n","    dataloader=torch.utils.data.DataLoader(triplets, batch_size=1, shuffle=False, num_workers=4, pin_memory=True)\n","    graphEmbs=torch.zeros((node_dim, num_timepoints))\n","    graphEmbs=self.forward(dataloader)\n","    regEmb=gRegressor(graphEmbs).squeeze(1)\n","    print(regEmb.shape)\n","    embs[:,i]=regEmb\n"],"metadata":{"id":"WN6apjKTdmOl"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}