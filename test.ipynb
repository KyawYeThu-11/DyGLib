{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "\n",
    "    def __init__(self, src_node_ids: np.ndarray, dst_node_ids: np.ndarray, node_interact_times: np.ndarray, edge_ids: np.ndarray, labels: np.ndarray):\n",
    "        \"\"\"\n",
    "        Data object to store the nodes interaction information.\n",
    "        :param src_node_ids: ndarray\n",
    "        :param dst_node_ids: ndarray\n",
    "        :param node_interact_times: ndarray\n",
    "        :param edge_ids: ndarray\n",
    "        :param labels: ndarray\n",
    "        \"\"\"\n",
    "        self.src_node_ids = src_node_ids\n",
    "        self.dst_node_ids = dst_node_ids\n",
    "        self.node_interact_times = node_interact_times\n",
    "        self.edge_ids = edge_ids\n",
    "        self.labels = labels\n",
    "        self.num_interactions = len(src_node_ids)\n",
    "        self.unique_node_ids = set(src_node_ids) | set(dst_node_ids)\n",
    "        self.num_unique_nodes = len(self.unique_node_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_df = pd.read_csv('./processed_data/{}/ml_{}.csv'.format('myket', 'myket'))\n",
    "val_time, test_time = list(np.quantile(graph_df.ts, [(1 - 0.15 - 0.15), (1 - 0.15)]))\n",
    "\n",
    "src_node_ids = graph_df.u.values.astype(np.longlong)\n",
    "dst_node_ids = graph_df.i.values.astype(np.longlong)\n",
    "node_interact_times = graph_df.ts.values.astype(np.float64)\n",
    "edge_ids = graph_df.idx.values.astype(np.longlong)\n",
    "labels = graph_df.label.values\n",
    "\n",
    "full_data = Data(src_node_ids=src_node_ids, dst_node_ids=dst_node_ids, node_interact_times=node_interact_times, edge_ids=edge_ids, labels=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_823652/3008281078.py:11: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  new_test_node_set = set(random.sample(test_node_set, int(0.1 * num_total_unique_node_ids)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 694121 interactions, involving 17988 different nodes\n",
      "The training dataset has 390580 interactions, involving 16098 different nodes\n",
      "The validation dataset has 104118 interactions, involving 15452 different nodes\n",
      "The test dataset has 104118 interactions, involving 14868 different nodes\n",
      "The new node validation dataset has 22986 interactions, involving 9475 different nodes\n",
      "The new node test dataset has 24437 interactions, involving 9479 different nodes\n",
      "1798 nodes were used for the inductive testing, i.e. are never seen during training\n"
     ]
    }
   ],
   "source": [
    "# the setting of seed follows previous works\n",
    "random.seed(2020)\n",
    "\n",
    "# union to get node set\n",
    "node_set = set(src_node_ids) | set(dst_node_ids)\n",
    "num_total_unique_node_ids = len(node_set)\n",
    "\n",
    "# compute nodes which appear at test time\n",
    "test_node_set = set(src_node_ids[node_interact_times > val_time]).union(set(dst_node_ids[node_interact_times > val_time]))\n",
    "# sample nodes which we keep as new nodes (to test inductiveness), so then we have to remove all their edges from training\n",
    "new_test_node_set = set(random.sample(test_node_set, int(0.1 * num_total_unique_node_ids)))\n",
    "\n",
    "# mask for each source and destination to denote whether they are new test nodes\n",
    "new_test_source_mask = graph_df.u.map(lambda x: x in new_test_node_set).values\n",
    "new_test_destination_mask = graph_df.i.map(lambda x: x in new_test_node_set).values\n",
    "\n",
    "# mask, which is true for edges with both destination and source not being new test nodes (because we want to remove all edges involving any new test node)\n",
    "observed_edges_mask = np.logical_and(~new_test_source_mask, ~new_test_destination_mask)\n",
    "\n",
    "# for train data, we keep edges happening before the validation time which do not involve any new node, used for inductiveness\n",
    "train_mask = np.logical_and(node_interact_times <= val_time, observed_edges_mask)\n",
    "\n",
    "train_data = Data(src_node_ids=src_node_ids[train_mask], dst_node_ids=dst_node_ids[train_mask],\n",
    "                      node_interact_times=node_interact_times[train_mask],\n",
    "                      edge_ids=edge_ids[train_mask], labels=labels[train_mask])\n",
    "\n",
    "# define the new nodes sets for testing inductiveness of the model\n",
    "train_node_set = set(train_data.src_node_ids).union(train_data.dst_node_ids)\n",
    "assert len(train_node_set & new_test_node_set) == 0\n",
    "    \n",
    "# new nodes that are not in the training set\n",
    "new_node_set = node_set - train_node_set\n",
    "\n",
    "val_mask = np.logical_and(node_interact_times <= test_time, node_interact_times > val_time)\n",
    "test_mask = node_interact_times > test_time\n",
    "\n",
    "# new edges with new nodes in the val and test set (for inductive evaluation)\n",
    "edge_contains_new_node_mask = np.array([(src_node_id in new_node_set or dst_node_id in new_node_set)\n",
    "                                            for src_node_id, dst_node_id in zip(src_node_ids, dst_node_ids)])\n",
    "new_node_val_mask = np.logical_and(val_mask, edge_contains_new_node_mask)\n",
    "new_node_test_mask = np.logical_and(test_mask, edge_contains_new_node_mask)\n",
    "\n",
    "# validation and test data\n",
    "val_data = Data(src_node_ids=src_node_ids[val_mask], dst_node_ids=dst_node_ids[val_mask],\n",
    "                    node_interact_times=node_interact_times[val_mask], edge_ids=edge_ids[val_mask], labels=labels[val_mask])\n",
    "\n",
    "test_data = Data(src_node_ids=src_node_ids[test_mask], dst_node_ids=dst_node_ids[test_mask],\n",
    "                     node_interact_times=node_interact_times[test_mask], edge_ids=edge_ids[test_mask], labels=labels[test_mask])\n",
    "\n",
    "# validation and test with edges that at least has one new node (not in training set)\n",
    "new_node_val_data = Data(src_node_ids=src_node_ids[new_node_val_mask], dst_node_ids=dst_node_ids[new_node_val_mask],\n",
    "                             node_interact_times=node_interact_times[new_node_val_mask],\n",
    "                             edge_ids=edge_ids[new_node_val_mask], labels=labels[new_node_val_mask])\n",
    "\n",
    "new_node_test_data = Data(src_node_ids=src_node_ids[new_node_test_mask], dst_node_ids=dst_node_ids[new_node_test_mask],\n",
    "                              node_interact_times=node_interact_times[new_node_test_mask],\n",
    "                              edge_ids=edge_ids[new_node_test_mask], labels=labels[new_node_test_mask])\n",
    "\n",
    "print(\"The dataset has {} interactions, involving {} different nodes\".format(full_data.num_interactions, full_data.num_unique_nodes))\n",
    "print(\"The training dataset has {} interactions, involving {} different nodes\".format(\n",
    "        train_data.num_interactions, train_data.num_unique_nodes))\n",
    "print(\"The validation dataset has {} interactions, involving {} different nodes\".format(\n",
    "        val_data.num_interactions, val_data.num_unique_nodes))\n",
    "print(\"The test dataset has {} interactions, involving {} different nodes\".format(\n",
    "        test_data.num_interactions, test_data.num_unique_nodes))\n",
    "print(\"The new node validation dataset has {} interactions, involving {} different nodes\".format(\n",
    "        new_node_val_data.num_interactions, new_node_val_data.num_unique_nodes))\n",
    "print(\"The new node test dataset has {} interactions, involving {} different nodes\".format(\n",
    "        new_node_test_data.num_interactions, new_node_test_data.num_unique_nodes))\n",
    "print(\"{} nodes were used for the inductive testing, i.e. are never seen during training\".format(len(new_test_node_set)))\n",
    "\n",
    "# return node_raw_features, edge_raw_features, full_data, train_data, val_data, test_data, new_node_val_data, new_node_test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
