{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import warnings\n",
    "import shutil\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from models.TGAT import TGAT\n",
    "from models.MemoryModel import MemoryModel, compute_src_dst_node_time_shifts\n",
    "from models.CAWN import CAWN\n",
    "from models.TCL import TCL\n",
    "from models.GraphMixer import GraphMixer\n",
    "from models.DyGFormer import DyGFormer\n",
    "from models.modules import MergeLayer\n",
    "from utils.utils import set_random_seed, convert_to_gpu, get_parameter_sizes, create_optimizer\n",
    "from utils.utils import get_neighbor_sampler, NegativeEdgeSampler\n",
    "from evaluate_models_utils import evaluate_model_link_prediction\n",
    "from utils.metrics import get_link_prediction_metrics\n",
    "from utils.DataLoader import get_idx_data_loader, get_link_prediction_data\n",
    "from utils.EarlyStopping import EarlyStopping\n",
    "from utils.load_configs import get_link_prediction_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, dataset_name):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.batch_size = 200\n",
    "        self.model_name = 'DyGFormer'\n",
    "        self.gpu = 0\n",
    "        self.num_neighbors = 20\n",
    "        self.sample_neighbor_strategy = 'recent'\n",
    "        self.time_scaling_factor = 1e-6\n",
    "        self.num_walk_heads = 8\n",
    "        self.num_heads = 2\n",
    "        self.num_layers = 2\n",
    "        self.walk_length = 1\n",
    "        self.time_gap = 2000\n",
    "        self.time_feat_dim = 100\n",
    "        self.position_feat_dim = 172\n",
    "        self.edge_bank_memory_mode = 'unlimited_memory'\n",
    "        self.time_window_mode = 'fixed_proportion'\n",
    "        self.patch_size = 1\n",
    "        self.channel_embedding_dim = 50\n",
    "        self.max_input_sequence_length = 32\n",
    "        self.learning_rate = 0.0001\n",
    "        self.dropout = 0.1\n",
    "        self.num_epochs = 100\n",
    "        self.optimizer = 'Adam'\n",
    "        self.weight_decay = 0.0\n",
    "        self.patience = 20\n",
    "        self.val_ratio = 0.15\n",
    "        self.test_ratio = 0.15\n",
    "        self.num_runs = 5\n",
    "        self.test_interval_epochs = 10\n",
    "        self.negative_sample_strategy = 'random'\n",
    "        self.load_best_configs = False\n",
    "\n",
    "# Create an instance of Args with the loaded configuration\n",
    "args = Args(dataset_name = 'train0')\n",
    "\n",
    "args.patch_size = 2\n",
    "args.max_input_sequence_length = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# get data for training, validation and testing\n",
    "node_raw_features, edge_raw_features, _, train_data, val_data, test_data, new_node_val_data, new_node_test_data = \\\n",
    "    get_link_prediction_data(dataset_name=args.dataset_name, val_ratio=args.val_ratio, test_ratio=args.test_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    # get arguments\n",
    "    args = get_link_prediction_args(is_evaluation=False)\n",
    "\n",
    "    # get data for training, validation and testing\n",
    "    node_raw_features, edge_raw_features, full_data, train_data, val_data, test_data, new_node_val_data, new_node_test_data = \\\n",
    "        get_link_prediction_data(dataset_name=args.dataset_name, val_ratio=args.val_ratio, test_ratio=args.test_ratio)\n",
    "\n",
    "    # initialize training neighbor sampler to retrieve temporal graph\n",
    "    train_neighbor_sampler = get_neighbor_sampler(data=train_data, sample_neighbor_strategy=args.sample_neighbor_strategy,\n",
    "                                                  time_scaling_factor=args.time_scaling_factor, seed=0)\n",
    "\n",
    "    # initialize validation and test neighbor sampler to retrieve temporal graph\n",
    "    full_neighbor_sampler = get_neighbor_sampler(data=full_data, sample_neighbor_strategy=args.sample_neighbor_strategy,\n",
    "                                                 time_scaling_factor=args.time_scaling_factor, seed=1)\n",
    "\n",
    "    # initialize negative samplers, set seeds for validation and testing so negatives are the same across different runs\n",
    "    # in the inductive setting, negatives are sampled only amongst other new nodes\n",
    "    # train negative edge sampler does not need to specify the seed, but evaluation samplers need to do so\n",
    "    train_neg_edge_sampler = NegativeEdgeSampler(src_node_ids=train_data.src_node_ids, dst_node_ids=train_data.dst_node_ids)\n",
    "    val_neg_edge_sampler = NegativeEdgeSampler(src_node_ids=full_data.src_node_ids, dst_node_ids=full_data.dst_node_ids, seed=0)\n",
    "    new_node_val_neg_edge_sampler = NegativeEdgeSampler(src_node_ids=new_node_val_data.src_node_ids, dst_node_ids=new_node_val_data.dst_node_ids, seed=1)\n",
    "    test_neg_edge_sampler = NegativeEdgeSampler(src_node_ids=full_data.src_node_ids, dst_node_ids=full_data.dst_node_ids, seed=2)\n",
    "    new_node_test_neg_edge_sampler = NegativeEdgeSampler(src_node_ids=new_node_test_data.src_node_ids, dst_node_ids=new_node_test_data.dst_node_ids, seed=3)\n",
    "\n",
    "    # get data loaders\n",
    "    train_idx_data_loader = get_idx_data_loader(indices_list=list(range(len(train_data.src_node_ids))), batch_size=args.batch_size, shuffle=False)\n",
    "    val_idx_data_loader = get_idx_data_loader(indices_list=list(range(len(val_data.src_node_ids))), batch_size=args.batch_size, shuffle=False)\n",
    "    new_node_val_idx_data_loader = get_idx_data_loader(indices_list=list(range(len(new_node_val_data.src_node_ids))), batch_size=args.batch_size, shuffle=False)\n",
    "    test_idx_data_loader = get_idx_data_loader(indices_list=list(range(len(test_data.src_node_ids))), batch_size=args.batch_size, shuffle=False)\n",
    "    new_node_test_idx_data_loader = get_idx_data_loader(indices_list=list(range(len(new_node_test_data.src_node_ids))), batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    val_metric_all_runs, new_node_val_metric_all_runs, test_metric_all_runs, new_node_test_metric_all_runs = [], [], [], []\n",
    "\n",
    "    for run in range(args.num_runs):\n",
    "\n",
    "        set_random_seed(seed=run)\n",
    "\n",
    "        args.seed = run\n",
    "        args.save_model_name = f'{args.model_name}_seed{args.seed}'\n",
    "\n",
    "        # set up logger\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        logger = logging.getLogger()\n",
    "        logger.setLevel(logging.DEBUG)\n",
    "        os.makedirs(f\"./logs/{args.model_name}/{args.dataset_name}/{args.save_model_name}/\", exist_ok=True)\n",
    "        # create file handler that logs debug and higher level messages\n",
    "        fh = logging.FileHandler(f\"./logs/{args.model_name}/{args.dataset_name}/{args.save_model_name}/{str(time.time())}.log\")\n",
    "        fh.setLevel(logging.DEBUG)\n",
    "        # create console handler with a higher log level\n",
    "        ch = logging.StreamHandler()\n",
    "        ch.setLevel(logging.WARNING)\n",
    "        # create formatter and add it to the handlers\n",
    "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "        fh.setFormatter(formatter)\n",
    "        ch.setFormatter(formatter)\n",
    "        # add the handlers to logger\n",
    "        logger.addHandler(fh)\n",
    "        logger.addHandler(ch)\n",
    "\n",
    "        run_start_time = time.time()\n",
    "        logger.info(f\"********** Run {run + 1} starts. **********\")\n",
    "\n",
    "        logger.info(f'configuration is {args}')\n",
    "\n",
    "        # create model\n",
    "        if args.model_name == 'TGAT':\n",
    "            dynamic_backbone = TGAT(node_raw_features=node_raw_features, edge_raw_features=edge_raw_features, neighbor_sampler=train_neighbor_sampler,\n",
    "                                    time_feat_dim=args.time_feat_dim, num_layers=args.num_layers, num_heads=args.num_heads, dropout=args.dropout, device=args.device)\n",
    "        elif args.model_name in ['JODIE', 'DyRep', 'TGN']:\n",
    "            # four floats that represent the mean and standard deviation of source and destination node time shifts in the training data, which is used for JODIE\n",
    "            src_node_mean_time_shift, src_node_std_time_shift, dst_node_mean_time_shift_dst, dst_node_std_time_shift = \\\n",
    "                compute_src_dst_node_time_shifts(train_data.src_node_ids, train_data.dst_node_ids, train_data.node_interact_times)\n",
    "            dynamic_backbone = MemoryModel(node_raw_features=node_raw_features, edge_raw_features=edge_raw_features, neighbor_sampler=train_neighbor_sampler,\n",
    "                                           time_feat_dim=args.time_feat_dim, model_name=args.model_name, num_layers=args.num_layers, num_heads=args.num_heads,\n",
    "                                           dropout=args.dropout, src_node_mean_time_shift=src_node_mean_time_shift, src_node_std_time_shift=src_node_std_time_shift,\n",
    "                                           dst_node_mean_time_shift_dst=dst_node_mean_time_shift_dst, dst_node_std_time_shift=dst_node_std_time_shift, device=args.device)\n",
    "        elif args.model_name == 'CAWN':\n",
    "            dynamic_backbone = CAWN(node_raw_features=node_raw_features, edge_raw_features=edge_raw_features, neighbor_sampler=train_neighbor_sampler,\n",
    "                                    time_feat_dim=args.time_feat_dim, position_feat_dim=args.position_feat_dim, walk_length=args.walk_length,\n",
    "                                    num_walk_heads=args.num_walk_heads, dropout=args.dropout, device=args.device)\n",
    "        elif args.model_name == 'TCL':\n",
    "            dynamic_backbone = TCL(node_raw_features=node_raw_features, edge_raw_features=edge_raw_features, neighbor_sampler=train_neighbor_sampler,\n",
    "                                   time_feat_dim=args.time_feat_dim, num_layers=args.num_layers, num_heads=args.num_heads,\n",
    "                                   num_depths=args.num_neighbors + 1, dropout=args.dropout, device=args.device)\n",
    "        elif args.model_name == 'GraphMixer':\n",
    "            dynamic_backbone = GraphMixer(node_raw_features=node_raw_features, edge_raw_features=edge_raw_features, neighbor_sampler=train_neighbor_sampler,\n",
    "                                          time_feat_dim=args.time_feat_dim, num_tokens=args.num_neighbors, num_layers=args.num_layers, dropout=args.dropout, device=args.device)\n",
    "        elif args.model_name == 'DyGFormer':\n",
    "            dynamic_backbone = DyGFormer(node_raw_features=node_raw_features, edge_raw_features=edge_raw_features, neighbor_sampler=train_neighbor_sampler,\n",
    "                                         time_feat_dim=args.time_feat_dim, channel_embedding_dim=args.channel_embedding_dim, patch_size=args.patch_size,\n",
    "                                         num_layers=args.num_layers, num_heads=args.num_heads, dropout=args.dropout,\n",
    "                                         max_input_sequence_length=args.max_input_sequence_length, device=args.device)\n",
    "        else:\n",
    "            raise ValueError(f\"Wrong value for model_name {args.model_name}!\")\n",
    "        link_predictor = MergeLayer(input_dim1=node_raw_features.shape[1], input_dim2=node_raw_features.shape[1],\n",
    "                                    hidden_dim=node_raw_features.shape[1], output_dim=1)\n",
    "        model = nn.Sequential(dynamic_backbone, link_predictor)\n",
    "        logger.info(f'model -> {model}')\n",
    "        logger.info(f'model name: {args.model_name}, #parameters: {get_parameter_sizes(model) * 4} B, '\n",
    "                    f'{get_parameter_sizes(model) * 4 / 1024} KB, {get_parameter_sizes(model) * 4 / 1024 / 1024} MB.')\n",
    "\n",
    "        optimizer = create_optimizer(model=model, optimizer_name=args.optimizer, learning_rate=args.learning_rate, weight_decay=args.weight_decay)\n",
    "\n",
    "        model = convert_to_gpu(model, device=args.device)\n",
    "\n",
    "        save_model_folder = f\"./saved_models/{args.model_name}/{args.dataset_name}/{args.save_model_name}/\"\n",
    "        shutil.rmtree(save_model_folder, ignore_errors=True)\n",
    "        os.makedirs(save_model_folder, exist_ok=True)\n",
    "\n",
    "        early_stopping = EarlyStopping(patience=args.patience, save_model_folder=save_model_folder,\n",
    "                                       save_model_name=args.save_model_name, logger=logger, model_name=args.model_name)\n",
    "\n",
    "        loss_func = nn.BCELoss()\n",
    "\n",
    "        for epoch in range(args.num_epochs):\n",
    "\n",
    "            model.train()\n",
    "            if args.model_name in ['DyRep', 'TGAT', 'TGN', 'CAWN', 'TCL', 'GraphMixer', 'DyGFormer']:\n",
    "                # training, only use training graph\n",
    "                model[0].set_neighbor_sampler(train_neighbor_sampler)\n",
    "            if args.model_name in ['JODIE', 'DyRep', 'TGN']:\n",
    "                # reinitialize memory of memory-based models at the start of each epoch\n",
    "                model[0].memory_bank.__init_memory_bank__()\n",
    "\n",
    "            # store train losses and metrics\n",
    "            train_losses, train_metrics = [], []\n",
    "            train_idx_data_loader_tqdm = tqdm(train_idx_data_loader, ncols=120)\n",
    "            for batch_idx, train_data_indices in enumerate(train_idx_data_loader_tqdm):\n",
    "                train_data_indices = train_data_indices.numpy()\n",
    "                batch_src_node_ids, batch_dst_node_ids, batch_node_interact_times, batch_edge_ids = \\\n",
    "                    train_data.src_node_ids[train_data_indices], train_data.dst_node_ids[train_data_indices], \\\n",
    "                    train_data.node_interact_times[train_data_indices], train_data.edge_ids[train_data_indices]\n",
    "\n",
    "                _, batch_neg_dst_node_ids = train_neg_edge_sampler.sample(size=len(batch_src_node_ids))\n",
    "                batch_neg_src_node_ids = batch_src_node_ids\n",
    "\n",
    "                # we need to compute for positive and negative edges respectively, because the new sampling strategy (for evaluation) allows the negative source nodes to be\n",
    "                # different from the source nodes, this is different from previous works that just replace destination nodes with negative destination nodes\n",
    "                if args.model_name in ['TGAT', 'CAWN', 'TCL']:\n",
    "                    # get temporal embedding of source and destination nodes\n",
    "                    # two Tensors, with shape (batch_size, node_feat_dim)\n",
    "                    batch_src_node_embeddings, batch_dst_node_embeddings = \\\n",
    "                        model[0].compute_src_dst_node_temporal_embeddings(src_node_ids=batch_src_node_ids,\n",
    "                                                                          dst_node_ids=batch_dst_node_ids,\n",
    "                                                                          node_interact_times=batch_node_interact_times,\n",
    "                                                                          num_neighbors=args.num_neighbors)\n",
    "\n",
    "                    # get temporal embedding of negative source and negative destination nodes\n",
    "                    # two Tensors, with shape (batch_size, node_feat_dim)\n",
    "                    batch_neg_src_node_embeddings, batch_neg_dst_node_embeddings = \\\n",
    "                        model[0].compute_src_dst_node_temporal_embeddings(src_node_ids=batch_neg_src_node_ids,\n",
    "                                                                          dst_node_ids=batch_neg_dst_node_ids,\n",
    "                                                                          node_interact_times=batch_node_interact_times,\n",
    "                                                                          num_neighbors=args.num_neighbors)\n",
    "                elif args.model_name in ['JODIE', 'DyRep', 'TGN']:\n",
    "                    # note that negative nodes do not change the memories while the positive nodes change the memories,\n",
    "                    # we need to first compute the embeddings of negative nodes for memory-based models\n",
    "                    # get temporal embedding of negative source and negative destination nodes\n",
    "                    # two Tensors, with shape (batch_size, node_feat_dim)\n",
    "                    batch_neg_src_node_embeddings, batch_neg_dst_node_embeddings = \\\n",
    "                        model[0].compute_src_dst_node_temporal_embeddings(src_node_ids=batch_neg_src_node_ids,\n",
    "                                                                          dst_node_ids=batch_neg_dst_node_ids,\n",
    "                                                                          node_interact_times=batch_node_interact_times,\n",
    "                                                                          edge_ids=None,\n",
    "                                                                          edges_are_positive=False,\n",
    "                                                                          num_neighbors=args.num_neighbors)\n",
    "\n",
    "                    # get temporal embedding of source and destination nodes\n",
    "                    # two Tensors, with shape (batch_size, node_feat_dim)\n",
    "                    batch_src_node_embeddings, batch_dst_node_embeddings = \\\n",
    "                        model[0].compute_src_dst_node_temporal_embeddings(src_node_ids=batch_src_node_ids,\n",
    "                                                                          dst_node_ids=batch_dst_node_ids,\n",
    "                                                                          node_interact_times=batch_node_interact_times,\n",
    "                                                                          edge_ids=batch_edge_ids,\n",
    "                                                                          edges_are_positive=True,\n",
    "                                                                          num_neighbors=args.num_neighbors)\n",
    "                elif args.model_name in ['GraphMixer']:\n",
    "                    # get temporal embedding of source and destination nodes\n",
    "                    # two Tensors, with shape (batch_size, node_feat_dim)\n",
    "                    batch_src_node_embeddings, batch_dst_node_embeddings = \\\n",
    "                        model[0].compute_src_dst_node_temporal_embeddings(src_node_ids=batch_src_node_ids,\n",
    "                                                                          dst_node_ids=batch_dst_node_ids,\n",
    "                                                                          node_interact_times=batch_node_interact_times,\n",
    "                                                                          num_neighbors=args.num_neighbors,\n",
    "                                                                          time_gap=args.time_gap)\n",
    "\n",
    "                    # get temporal embedding of negative source and negative destination nodes\n",
    "                    # two Tensors, with shape (batch_size, node_feat_dim)\n",
    "                    batch_neg_src_node_embeddings, batch_neg_dst_node_embeddings = \\\n",
    "                        model[0].compute_src_dst_node_temporal_embeddings(src_node_ids=batch_neg_src_node_ids,\n",
    "                                                                          dst_node_ids=batch_neg_dst_node_ids,\n",
    "                                                                          node_interact_times=batch_node_interact_times,\n",
    "                                                                          num_neighbors=args.num_neighbors,\n",
    "                                                                          time_gap=args.time_gap)\n",
    "                elif args.model_name in ['DyGFormer']:\n",
    "                    # get temporal embedding of source and destination nodes\n",
    "                    # two Tensors, with shape (batch_size, node_feat_dim)\n",
    "                    batch_src_node_embeddings, batch_dst_node_embeddings = \\\n",
    "                        model[0].compute_src_dst_node_temporal_embeddings(src_node_ids=batch_src_node_ids,\n",
    "                                                                          dst_node_ids=batch_dst_node_ids,\n",
    "                                                                          node_interact_times=batch_node_interact_times)\n",
    "\n",
    "                    # get temporal embedding of negative source and negative destination nodes\n",
    "                    # two Tensors, with shape (batch_size, node_feat_dim)\n",
    "                    batch_neg_src_node_embeddings, batch_neg_dst_node_embeddings = \\\n",
    "                        model[0].compute_src_dst_node_temporal_embeddings(src_node_ids=batch_neg_src_node_ids,\n",
    "                                                                          dst_node_ids=batch_neg_dst_node_ids,\n",
    "                                                                          node_interact_times=batch_node_interact_times)\n",
    "                else:\n",
    "                    raise ValueError(f\"Wrong value for model_name {args.model_name}!\")\n",
    "                # get positive and negative probabilities, shape (batch_size, )\n",
    "                positive_probabilities = model[1](input_1=batch_src_node_embeddings, input_2=batch_dst_node_embeddings).squeeze(dim=-1).sigmoid()\n",
    "                negative_probabilities = model[1](input_1=batch_neg_src_node_embeddings, input_2=batch_neg_dst_node_embeddings).squeeze(dim=-1).sigmoid()\n",
    "\n",
    "                predicts = torch.cat([positive_probabilities, negative_probabilities], dim=0)\n",
    "                labels = torch.cat([torch.ones_like(positive_probabilities), torch.zeros_like(negative_probabilities)], dim=0)\n",
    "\n",
    "                loss = loss_func(input=predicts, target=labels)\n",
    "\n",
    "                train_losses.append(loss.item())\n",
    "\n",
    "                train_metrics.append(get_link_prediction_metrics(predicts=predicts, labels=labels))\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_idx_data_loader_tqdm.set_description(f'Epoch: {epoch + 1}, train for the {batch_idx + 1}-th batch, train loss: {loss.item()}')\n",
    "\n",
    "                if args.model_name in ['JODIE', 'DyRep', 'TGN']:\n",
    "                    # detach the memories and raw messages of nodes in the memory bank after each batch, so we don't back propagate to the start of time\n",
    "                    model[0].memory_bank.detach_memory_bank()\n",
    "\n",
    "            if args.model_name in ['JODIE', 'DyRep', 'TGN']:\n",
    "                # backup memory bank after training so it can be used for new validation nodes\n",
    "                train_backup_memory_bank = model[0].memory_bank.backup_memory_bank()\n",
    "\n",
    "            val_losses, val_metrics = evaluate_model_link_prediction(model_name=args.model_name,\n",
    "                                                                     model=model,\n",
    "                                                                     neighbor_sampler=full_neighbor_sampler,\n",
    "                                                                     evaluate_idx_data_loader=val_idx_data_loader,\n",
    "                                                                     evaluate_neg_edge_sampler=val_neg_edge_sampler,\n",
    "                                                                     evaluate_data=val_data,\n",
    "                                                                     loss_func=loss_func,\n",
    "                                                                     num_neighbors=args.num_neighbors,\n",
    "                                                                     time_gap=args.time_gap)\n",
    "\n",
    "            if args.model_name in ['JODIE', 'DyRep', 'TGN']:\n",
    "                # backup memory bank after validating so it can be used for testing nodes (since test edges are strictly later in time than validation edges)\n",
    "                val_backup_memory_bank = model[0].memory_bank.backup_memory_bank()\n",
    "\n",
    "                # reload training memory bank for new validation nodes\n",
    "                model[0].memory_bank.reload_memory_bank(train_backup_memory_bank)\n",
    "\n",
    "            new_node_val_losses, new_node_val_metrics = evaluate_model_link_prediction(model_name=args.model_name,\n",
    "                                                                                       model=model,\n",
    "                                                                                       neighbor_sampler=full_neighbor_sampler,\n",
    "                                                                                       evaluate_idx_data_loader=new_node_val_idx_data_loader,\n",
    "                                                                                       evaluate_neg_edge_sampler=new_node_val_neg_edge_sampler,\n",
    "                                                                                       evaluate_data=new_node_val_data,\n",
    "                                                                                       loss_func=loss_func,\n",
    "                                                                                       num_neighbors=args.num_neighbors,\n",
    "                                                                                       time_gap=args.time_gap)\n",
    "\n",
    "            if args.model_name in ['JODIE', 'DyRep', 'TGN']:\n",
    "                # reload validation memory bank for testing nodes or saving models\n",
    "                # note that since model treats memory as parameters, we need to reload the memory to val_backup_memory_bank for saving models\n",
    "                model[0].memory_bank.reload_memory_bank(val_backup_memory_bank)\n",
    "\n",
    "            logger.info(f'Epoch: {epoch + 1}, learning rate: {optimizer.param_groups[0][\"lr\"]}, train loss: {np.mean(train_losses):.4f}')\n",
    "            for metric_name in train_metrics[0].keys():\n",
    "                logger.info(f'train {metric_name}, {np.mean([train_metric[metric_name] for train_metric in train_metrics]):.4f}')\n",
    "            logger.info(f'validate loss: {np.mean(val_losses):.4f}')\n",
    "            for metric_name in val_metrics[0].keys():\n",
    "                logger.info(f'validate {metric_name}, {np.mean([val_metric[metric_name] for val_metric in val_metrics]):.4f}')\n",
    "            logger.info(f'new node validate loss: {np.mean(new_node_val_losses):.4f}')\n",
    "            for metric_name in new_node_val_metrics[0].keys():\n",
    "                logger.info(f'new node validate {metric_name}, {np.mean([new_node_val_metric[metric_name] for new_node_val_metric in new_node_val_metrics]):.4f}')\n",
    "\n",
    "            # perform testing once after test_interval_epochs\n",
    "            if (epoch + 1) % args.test_interval_epochs == 0:\n",
    "                test_losses, test_metrics = evaluate_model_link_prediction(model_name=args.model_name,\n",
    "                                                                           model=model,\n",
    "                                                                           neighbor_sampler=full_neighbor_sampler,\n",
    "                                                                           evaluate_idx_data_loader=test_idx_data_loader,\n",
    "                                                                           evaluate_neg_edge_sampler=test_neg_edge_sampler,\n",
    "                                                                           evaluate_data=test_data,\n",
    "                                                                           loss_func=loss_func,\n",
    "                                                                           num_neighbors=args.num_neighbors,\n",
    "                                                                           time_gap=args.time_gap)\n",
    "\n",
    "                if args.model_name in ['JODIE', 'DyRep', 'TGN']:\n",
    "                    # reload validation memory bank for new testing nodes\n",
    "                    model[0].memory_bank.reload_memory_bank(val_backup_memory_bank)\n",
    "\n",
    "                new_node_test_losses, new_node_test_metrics = evaluate_model_link_prediction(model_name=args.model_name,\n",
    "                                                                                             model=model,\n",
    "                                                                                             neighbor_sampler=full_neighbor_sampler,\n",
    "                                                                                             evaluate_idx_data_loader=new_node_test_idx_data_loader,\n",
    "                                                                                             evaluate_neg_edge_sampler=new_node_test_neg_edge_sampler,\n",
    "                                                                                             evaluate_data=new_node_test_data,\n",
    "                                                                                             loss_func=loss_func,\n",
    "                                                                                             num_neighbors=args.num_neighbors,\n",
    "                                                                                             time_gap=args.time_gap)\n",
    "\n",
    "                if args.model_name in ['JODIE', 'DyRep', 'TGN']:\n",
    "                    # reload validation memory bank for testing nodes or saving models\n",
    "                    # note that since model treats memory as parameters, we need to reload the memory to val_backup_memory_bank for saving models\n",
    "                    model[0].memory_bank.reload_memory_bank(val_backup_memory_bank)\n",
    "\n",
    "                logger.info(f'test loss: {np.mean(test_losses):.4f}')\n",
    "                for metric_name in test_metrics[0].keys():\n",
    "                    logger.info(f'test {metric_name}, {np.mean([test_metric[metric_name] for test_metric in test_metrics]):.4f}')\n",
    "                logger.info(f'new node test loss: {np.mean(new_node_test_losses):.4f}')\n",
    "                for metric_name in new_node_test_metrics[0].keys():\n",
    "                    logger.info(f'new node test {metric_name}, {np.mean([new_node_test_metric[metric_name] for new_node_test_metric in new_node_test_metrics]):.4f}')\n",
    "\n",
    "            # select the best model based on all the validate metrics\n",
    "            val_metric_indicator = []\n",
    "            for metric_name in val_metrics[0].keys():\n",
    "                val_metric_indicator.append((metric_name, np.mean([val_metric[metric_name] for val_metric in val_metrics]), True))\n",
    "            early_stop = early_stopping.step(val_metric_indicator, model)\n",
    "\n",
    "            if early_stop:\n",
    "                break\n",
    "\n",
    "        # load the best model\n",
    "        early_stopping.load_checkpoint(model)\n",
    "\n",
    "        # evaluate the best model\n",
    "        logger.info(f'get final performance on dataset {args.dataset_name}...')\n",
    "\n",
    "        # the saved best model of memory-based models cannot perform validation since the stored memory has been updated by validation data\n",
    "        if args.model_name not in ['JODIE', 'DyRep', 'TGN']:\n",
    "            val_losses, val_metrics = evaluate_model_link_prediction(model_name=args.model_name,\n",
    "                                                                     model=model,\n",
    "                                                                     neighbor_sampler=full_neighbor_sampler,\n",
    "                                                                     evaluate_idx_data_loader=val_idx_data_loader,\n",
    "                                                                     evaluate_neg_edge_sampler=val_neg_edge_sampler,\n",
    "                                                                     evaluate_data=val_data,\n",
    "                                                                     loss_func=loss_func,\n",
    "                                                                     num_neighbors=args.num_neighbors,\n",
    "                                                                     time_gap=args.time_gap)\n",
    "\n",
    "            new_node_val_losses, new_node_val_metrics = evaluate_model_link_prediction(model_name=args.model_name,\n",
    "                                                                                       model=model,\n",
    "                                                                                       neighbor_sampler=full_neighbor_sampler,\n",
    "                                                                                       evaluate_idx_data_loader=new_node_val_idx_data_loader,\n",
    "                                                                                       evaluate_neg_edge_sampler=new_node_val_neg_edge_sampler,\n",
    "                                                                                       evaluate_data=new_node_val_data,\n",
    "                                                                                       loss_func=loss_func,\n",
    "                                                                                       num_neighbors=args.num_neighbors,\n",
    "                                                                                       time_gap=args.time_gap)\n",
    "\n",
    "        if args.model_name in ['JODIE', 'DyRep', 'TGN']:\n",
    "            # the memory in the best model has seen the validation edges, we need to backup the memory for new testing nodes\n",
    "            val_backup_memory_bank = model[0].memory_bank.backup_memory_bank()\n",
    "\n",
    "        test_losses, test_metrics = evaluate_model_link_prediction(model_name=args.model_name,\n",
    "                                                                   model=model,\n",
    "                                                                   neighbor_sampler=full_neighbor_sampler,\n",
    "                                                                   evaluate_idx_data_loader=test_idx_data_loader,\n",
    "                                                                   evaluate_neg_edge_sampler=test_neg_edge_sampler,\n",
    "                                                                   evaluate_data=test_data,\n",
    "                                                                   loss_func=loss_func,\n",
    "                                                                   num_neighbors=args.num_neighbors,\n",
    "                                                                   time_gap=args.time_gap)\n",
    "\n",
    "        if args.model_name in ['JODIE', 'DyRep', 'TGN']:\n",
    "            # reload validation memory bank for new testing nodes\n",
    "            model[0].memory_bank.reload_memory_bank(val_backup_memory_bank)\n",
    "\n",
    "        new_node_test_losses, new_node_test_metrics = evaluate_model_link_prediction(model_name=args.model_name,\n",
    "                                                                                     model=model,\n",
    "                                                                                     neighbor_sampler=full_neighbor_sampler,\n",
    "                                                                                     evaluate_idx_data_loader=new_node_test_idx_data_loader,\n",
    "                                                                                     evaluate_neg_edge_sampler=new_node_test_neg_edge_sampler,\n",
    "                                                                                     evaluate_data=new_node_test_data,\n",
    "                                                                                     loss_func=loss_func,\n",
    "                                                                                     num_neighbors=args.num_neighbors,\n",
    "                                                                                     time_gap=args.time_gap)\n",
    "        # store the evaluation metrics at the current run\n",
    "        val_metric_dict, new_node_val_metric_dict, test_metric_dict, new_node_test_metric_dict = {}, {}, {}, {}\n",
    "\n",
    "        if args.model_name not in ['JODIE', 'DyRep', 'TGN']:\n",
    "            logger.info(f'validate loss: {np.mean(val_losses):.4f}')\n",
    "            for metric_name in val_metrics[0].keys():\n",
    "                average_val_metric = np.mean([val_metric[metric_name] for val_metric in val_metrics])\n",
    "                logger.info(f'validate {metric_name}, {average_val_metric:.4f}')\n",
    "                val_metric_dict[metric_name] = average_val_metric\n",
    "\n",
    "            logger.info(f'new node validate loss: {np.mean(new_node_val_losses):.4f}')\n",
    "            for metric_name in new_node_val_metrics[0].keys():\n",
    "                average_new_node_val_metric = np.mean([new_node_val_metric[metric_name] for new_node_val_metric in new_node_val_metrics])\n",
    "                logger.info(f'new node validate {metric_name}, {average_new_node_val_metric:.4f}')\n",
    "                new_node_val_metric_dict[metric_name] = average_new_node_val_metric\n",
    "\n",
    "        logger.info(f'test loss: {np.mean(test_losses):.4f}')\n",
    "        for metric_name in test_metrics[0].keys():\n",
    "            average_test_metric = np.mean([test_metric[metric_name] for test_metric in test_metrics])\n",
    "            logger.info(f'test {metric_name}, {average_test_metric:.4f}')\n",
    "            test_metric_dict[metric_name] = average_test_metric\n",
    "\n",
    "        logger.info(f'new node test loss: {np.mean(new_node_test_losses):.4f}')\n",
    "        for metric_name in new_node_test_metrics[0].keys():\n",
    "            average_new_node_test_metric = np.mean([new_node_test_metric[metric_name] for new_node_test_metric in new_node_test_metrics])\n",
    "            logger.info(f'new node test {metric_name}, {average_new_node_test_metric:.4f}')\n",
    "            new_node_test_metric_dict[metric_name] = average_new_node_test_metric\n",
    "\n",
    "        single_run_time = time.time() - run_start_time\n",
    "        logger.info(f'Run {run + 1} cost {single_run_time:.2f} seconds.')\n",
    "\n",
    "        if args.model_name not in ['JODIE', 'DyRep', 'TGN']:\n",
    "            val_metric_all_runs.append(val_metric_dict)\n",
    "            new_node_val_metric_all_runs.append(new_node_val_metric_dict)\n",
    "        test_metric_all_runs.append(test_metric_dict)\n",
    "        new_node_test_metric_all_runs.append(new_node_test_metric_dict)\n",
    "\n",
    "        # avoid the overlap of logs\n",
    "        if run < args.num_runs - 1:\n",
    "            logger.removeHandler(fh)\n",
    "            logger.removeHandler(ch)\n",
    "\n",
    "        # save model result\n",
    "        if args.model_name not in ['JODIE', 'DyRep', 'TGN']:\n",
    "            result_json = {\n",
    "                \"validate metrics\": {metric_name: f'{val_metric_dict[metric_name]:.4f}' for metric_name in val_metric_dict},\n",
    "                \"new node validate metrics\": {metric_name: f'{new_node_val_metric_dict[metric_name]:.4f}' for metric_name in new_node_val_metric_dict},\n",
    "                \"test metrics\": {metric_name: f'{test_metric_dict[metric_name]:.4f}' for metric_name in test_metric_dict},\n",
    "                \"new node test metrics\": {metric_name: f'{new_node_test_metric_dict[metric_name]:.4f}' for metric_name in new_node_test_metric_dict}\n",
    "            }\n",
    "        else:\n",
    "            result_json = {\n",
    "                \"test metrics\": {metric_name: f'{test_metric_dict[metric_name]:.4f}' for metric_name in test_metric_dict},\n",
    "                \"new node test metrics\": {metric_name: f'{new_node_test_metric_dict[metric_name]:.4f}' for metric_name in new_node_test_metric_dict}\n",
    "            }\n",
    "        result_json = json.dumps(result_json, indent=4)\n",
    "\n",
    "        save_result_folder = f\"./saved_results/{args.model_name}/{args.dataset_name}\"\n",
    "        os.makedirs(save_result_folder, exist_ok=True)\n",
    "        save_result_path = os.path.join(save_result_folder, f\"{args.save_model_name}.json\")\n",
    "\n",
    "        with open(save_result_path, 'w') as file:\n",
    "            file.write(result_json)\n",
    "\n",
    "    # store the average metrics at the log of the last run\n",
    "    logger.info(f'metrics over {args.num_runs} runs:')\n",
    "\n",
    "    if args.model_name not in ['JODIE', 'DyRep', 'TGN']:\n",
    "        for metric_name in val_metric_all_runs[0].keys():\n",
    "            logger.info(f'validate {metric_name}, {[val_metric_single_run[metric_name] for val_metric_single_run in val_metric_all_runs]}')\n",
    "            logger.info(f'average validate {metric_name}, {np.mean([val_metric_single_run[metric_name] for val_metric_single_run in val_metric_all_runs]):.4f} '\n",
    "                        f'± {np.std([val_metric_single_run[metric_name] for val_metric_single_run in val_metric_all_runs], ddof=1):.4f}')\n",
    "\n",
    "        for metric_name in new_node_val_metric_all_runs[0].keys():\n",
    "            logger.info(f'new node validate {metric_name}, {[new_node_val_metric_single_run[metric_name] for new_node_val_metric_single_run in new_node_val_metric_all_runs]}')\n",
    "            logger.info(f'average new node validate {metric_name}, {np.mean([new_node_val_metric_single_run[metric_name] for new_node_val_metric_single_run in new_node_val_metric_all_runs]):.4f} '\n",
    "                        f'± {np.std([new_node_val_metric_single_run[metric_name] for new_node_val_metric_single_run in new_node_val_metric_all_runs], ddof=1):.4f}')\n",
    "\n",
    "    for metric_name in test_metric_all_runs[0].keys():\n",
    "        logger.info(f'test {metric_name}, {[test_metric_single_run[metric_name] for test_metric_single_run in test_metric_all_runs]}')\n",
    "        logger.info(f'average test {metric_name}, {np.mean([test_metric_single_run[metric_name] for test_metric_single_run in test_metric_all_runs]):.4f} '\n",
    "                    f'± {np.std([test_metric_single_run[metric_name] for test_metric_single_run in test_metric_all_runs], ddof=1):.4f}')\n",
    "\n",
    "    for metric_name in new_node_test_metric_all_runs[0].keys():\n",
    "        logger.info(f'new node test {metric_name}, {[new_node_test_metric_single_run[metric_name] for new_node_test_metric_single_run in new_node_test_metric_all_runs]}')\n",
    "        logger.info(f'average new node test {metric_name}, {np.mean([new_node_test_metric_single_run[metric_name] for new_node_test_metric_single_run in new_node_test_metric_all_runs]):.4f} '\n",
    "                    f'± {np.std([new_node_test_metric_single_run[metric_name] for new_node_test_metric_single_run in new_node_test_metric_all_runs], ddof=1):.4f}')\n",
    "\n",
    "    sys.exit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
